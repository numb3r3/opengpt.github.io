{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/","title":"Introduction","text":"<p><code>opengpt</code> is an open-source cloud-native large-scale multimodal models (LMMs) serving framework.  It is designed to simplify the deployment and management of large language models, on a distributed cluster of GPUs. We aim to make it a one-stop solution for a centralized and accessible place to gather techniques for optimizing large-scale multimodal models and make them easy to use for everyone.</p>"},{"location":"docs/#installation","title":"Installation","text":"<p>To use <code>opengpt</code>, install it with <code>pip</code>:</p> <pre><code>$ pip install open_gpt_torch\n</code></pre> <p>NOTE:</p> <p>To run open_gpt locally, it is required to have <code>Pytorch</code> pre-installed (see Pytorch installation guide).</p>"},{"location":"docs/#quick-start","title":"Quick Start","text":"<p>We use the <code>stabilityai/stablelm-tuned-alpha-3b</code> model from the huggingface as the open example as it is relatively small and fast to download.</p> <pre><code>import open_gpt\n\nmodel = open_gpt.create_model(\n    'stabilityai/stablelm-tuned-alpha-3b', device='cuda', precision='fp16'\n)\n\nprompt = \"The quick brown fox jumps over the lazy dog.\"\n\noutput = model.generate(\n    prompt,\n    max_length=100,\n    temperature=0.9,\n    top_k=50,\n    top_p=0.95,\n    repetition_penalty=1.2,\n    do_sample=True,\n)\n</code></pre> <p>In the above example, we use <code>precision='fp16'</code> to reduce the memory footprint and speed up the inference  with some loss in accuracy on text generation tasks. You can also use <code>precision='fp32'</code> instead as you like for better performance.</p> <p>NOTE:</p> <p>It usually takes a while (several minutes) for the first time to download and load the model into the memory.</p>"},{"location":"docs/#serving-models","title":"Serving Models","text":"<p>The <code>opengpt</code> package provides a simple and unified API for serving large models.  You can use it to serve your own models without any extra effort, and start to serve your models with <code>serve</code> CLI simply by:</p> <pre><code>$ opengpt serve stabilityai/stablelm-tuned-alpha-3b \\\n--device cuda \\\n--precision fp16\n</code></pre> <p>Once the server is ready, you will see the following logs:</p> <p></p> <p>As you can see, this will start a gRPC and HTTP server listening on <code>0.0.0.0:51001</code> and <code>0.0.0.0:51002</code> respectively by default. </p> <p>NOTE:</p> <p>You can inspect the available options with <code>opengpt serve --help</code>.</p> <p>Then, you can access the model server with gRPC or HTTP API depending on your needs.</p>"},{"location":"docs/#grpc-client-api","title":"gRPC Client API","text":"<p>To use the gRPC API, you can use the <code>opengpt.Client</code> module to connect to the model server:</p> <pre><code>from open_gpt import Client\n\nclient = Client()\n\n# connect to the model server\nmodel = client.get_model(endpoint='grpc://0.0.0.0:51001')\n\nprompt = \"The quick brown fox jumps over the lazy dog.\"\n\noutput = model.generate(\n    prompt,\n    max_length=100,\n    temperature=0.9,\n    top_k=50,\n    top_p=0.95,\n    repetition_penalty=1.2,\n    do_sample=True,\n    num_return_sequences=1,\n)\n</code></pre>"},{"location":"docs/#http-client-api","title":"HTTP Client API","text":"<p>To use the HTTP API, you can simply send a <code>POST</code> request to the <code>/generate</code> endpoint:</p> <pre><code>import requests\n\nprompt = \"The quick brown fox jumps over the lazy dog.\"\n\nresponse = requests.post(\n    \"http://0.0.0.0:51002/generate\",\n    json={\n        \"prompt\": prompt,\n        \"max_length\": 100,\n        \"temperature\": 0.9,\n        \"top_k\": 50,\n        \"top_p\": 0.95,\n        \"repetition_penalty\": 1.2,\n        \"do_sample\": True,\n        \"num_return_sequences\": 1,\n    },\n)\n</code></pre>"},{"location":"docs/deployment/clouds/","title":"Clouds","text":"<p>NOTE:</p> <p>If you want to use <code>dstack</code> alone, it's enough to start the Hub server locally using the <code>dstack start</code> command. It will allow you to run dev environments, pipelines, and apps both locally and in the cloud.</p> <p>However, if you want to work as a team and manage cloud credentials in a more secure way, you may consider deploying the Hub server into your cloud.</p> <p>Currently, the easiest way to deploy the Hub server to your cloud is by using the Docker image.</p> <p>NOTE:</p> <p>More detailed instructions on how to deploy to each specific cloud, including the Terraform templates and instructions for deploying via Kubernetes, will be coming soon.</p> <p>Meanwhile, if you need help, please reach out to us on Slack.</p>"},{"location":"docs/deployment/docker/","title":"Docker","text":"<p>NOTE:</p> <p>As an alternative to the <code>dstack start</code> command,  you can run the Hub server via Docker. This is recommended if you want to deploy the Hub server in an environment that supports Docker.</p> <p>Here's how to run <code>dstack</code> via Docker:</p> <pre><code>$ docker run --name dstack -p &amp;lt;port-on-host&amp;gt;:3000 \\ \n-v &amp;ltpath-to-data-directory&amp;gt;:/root/.dstack/hub \\\ndstackai/dstack\n</code></pre>"},{"location":"docs/deployment/docker/#environment-variables","title":"Environment variables","text":"<p>Here's the list of environment variables which you can override:</p> <ul> <li><code>DSTACK_HUB_ADMIN_TOKEN</code> \u2013 (Optional) The default token of the <code>admin</code> user. By default, it's generated randomly   at the first startup.</li> <li><code>DSTACK_HUB_DATA</code> \u2013 (Optional) The path to the directory where the Hub server stores the state. Defaults to <code>~/.dstack/hub/data</code>.</li> </ul>"},{"location":"docs/deployment/docker/#persisting-state","title":"Persisting state","text":"<p>By default, <code>dstack</code> saves state in a local directory (see <code>DSTACK_HUB_DATA</code>). If you want to persist state automatically to a cloud object storage, you can configure the following environment variables.</p> <ul> <li><code>LITESTREAM_REPLICA_URL</code> - The url of the cloud object storage.   Examples: <code>s3://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>gcs://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>abs://&lt;storage-account&gt;@&lt;container-name&gt;/&lt;path&gt;</code>, etc.</li> </ul> AWS S3 <p>To persist state into an AWS S3 bucket, provide the following environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> - The AWS access key ID</li> <li><code>AWS_SECRET_ACCESS_KEY</code> -  The AWS secret access key</li> </ul> GCP Storage <p>To persist state into an AWS S3 bucket, provide one of the following environment variables:</p> <ul> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code> - The path to the GCP service account key JSON file</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> - The GCP service account key JSON</li> </ul> Azure Blob Storage <p>To persist state into an Azure blog storage, provide the following environment variable.</p> <ul> <li><code>LITESTREAM_AZURE_ACCOUNT_KEY</code> - The Azure storage account key</li> </ul> <p>More details on options for configuring persistent state.</p>"},{"location":"docs/deployment/docker/#limitation","title":"Limitation","text":"<p>When you run <code>dstack</code> via Docker, it doesn't allow you to configure a project that runs dev environments, pipelines, and apps locally. You can only configure the projects that run dev environments, pipelines, and apps in the cloud. </p> <p>NOTE:</p> <p>If you want <code>dstack</code> to run dev environments, pipelines, and apps both locally and in the cloud, it is recommended to start the Hub server using the <code>dstack start</code> command.</p>"},{"location":"docs/deployment/hf-spaces/","title":"Hugging Face Spaces","text":"<p>NOTE:</p> <p>If you want to use <code>dstack</code> alone, it's enough to start the Hub server locally using the <code>dstack start</code> command. It will allow you to run dev environments, pipelines, and apps both locally and in the cloud.</p> <p>However, if you want to work as a team and manage cloud credentials in a more secure way, you may consider deploying the Hub server as a Hugging Face Space.</p>"},{"location":"docs/deployment/hf-spaces/#create-a-space","title":"Create a space","text":"<p>NOTE:</p> <p>Before creating a Hugging Face Space, make sure you're logged in to Hugging Face.</p> <p>The easiest way to create a Space to run <code>dstack</code> is duplicating the <code>dstackai/dstack-template</code> Space template by clicking the button below.</p> <p>Create a Space</p>"},{"location":"docs/deployment/hf-spaces/#configure-the-settings","title":"Configure the settings","text":"<p>Once the space is created, click on <code>Settings</code> and proceed to configure the visibility and secrets of the space.</p>"},{"location":"docs/deployment/hf-spaces/#visibility","title":"Visibility","text":"<p>Ensure that the visibility of the space is set to <code>Public</code>. This is a requirement for the <code>dstack</code> CLI to access the Hub server APIs. Rest assured, the Hub server has its own authentication system and will only allow registered users to access it.</p>"},{"location":"docs/deployment/hf-spaces/#secrets","title":"Secrets","text":""},{"location":"docs/deployment/hf-spaces/#token","title":"Token","text":"<p>If you want to configure the value of the default token for the admin user, add the <code>DSTACK_HUB_ADMIN_TOKEN</code> secret with the required value.</p> <p>If you don't do that, dstack will generate it randomly and print it to the <code>Logs</code>.</p>"},{"location":"docs/deployment/hf-spaces/#persisting-state","title":"Persisting state","text":"<p>If you want the space to maintain the state across restarts, you need to add the secrets that instruct <code>dstack</code> to persist the state in a cloud object storage of your choice.</p> <ul> <li><code>LITESTREAM_REPLICA_URL</code> - The url of the cloud object storage.   Examples: <code>s3://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>gcs://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>abs://&lt;storage-account&gt;@&lt;container-name&gt;/&lt;path&gt;</code>, etc.</li> </ul> AWS S3 <p>To persist state into an AWS S3 bucket, provide the following environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> - The AWS access key ID</li> <li><code>AWS_SECRET_ACCESS_KEY</code> -  The AWS secret access key</li> </ul> GCP Storage <p>To persist state into an AWS S3 bucket, provide one of the following environment variables:</p> <ul> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code> - The path to the GCP service account key JSON file</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> - The GCP service account key JSON</li> </ul> Azure Blob Storage <p>To persist state into an Azure blog storage, provide the following environment variable.</p> <ul> <li><code>LITESTREAM_AZURE_ACCOUNT_KEY</code> - The Azure storage account key</li> </ul> <p>Once the settings are configured, you can restart the space and log in as an administrator. From there, you can create users, projects, and configure the <code>dstack</code> CLI. This allows you to collaborate as a team, running development environments, pipelines, and apps securely and conveniently in the cloud without the need to host the Hub server yourself.</p>"},{"location":"docs/guides/apps/","title":"Apps","text":"<p>An app can be either a web application (such as Streamlit, Gradio, etc.) or an API endpoint (like FastAPI, Flask, etc.) setup based on a pre-defined configuration.</p> <p>With <code>dstack</code>, you can define such configurations as code and run your apps with a single command, either locally or in any cloud.</p>"},{"location":"docs/guides/apps/#creating-a-configuration-file","title":"Creating a configuration file","text":"<p>A configuration can be defined as a YAML file (under the <code>.dstack/workflows</code> directory).</p> <pre><code>workflows:\n- name: fastapi-gpu\nprovider: bash\nports:\n- 3000\ncommands:\n- pip install -r apps/requirements.txt\n- uvicorn apps.main:app --port 3000 --host 0.0.0.0\nresources:\ngpu:\ncount: 1\n</code></pre> <p>The configuration allows you to customize hardware resources, set up the Python environment,  and more.</p> <p>To configure ports, you have to specify the list of ports via the  <code>ports</code> property.</p>"},{"location":"docs/guides/apps/#running-an-app","title":"Running an app","text":"<p>Once a configuration is defined, you can run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run fastapi-gpu\n RUN           WORKFLOW     SUBMITTED  STATUS     TAG\n silly-dodo-1  fastapi-gpu  now        Submitted     \n\nStarting SSH tunnel...\n\nTo interrupt, press Ctrl+C.\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:3000 (Press CTRL+C to quit)\n</code></pre> <p>For convenience, <code>dstack</code> uses an exact copy of the source code that is locally present in the folder where you use the <code>dstack</code> command.</p> Using .gitignore <p>If you don't want the app to sync certain files (especially large local files that are not needed for the app), feel free to add them to the <code>.gitignore</code> file. In this case, <code>dstack</code> will ignore them, even if you aren't using Git.</p> <p>If you configure a project to run apps in the cloud, <code>dstack</code> will automatically provision the required cloud resources, and forward ports of the app to your local machine.</p> <p>NOTE:</p> <p>By default, <code>dstack</code> tries to map the application ports to the same local ports. However, you can specify which local ports to map the application ports to. For example, if you want to access the application port <code>3000</code>  on <code>127.0.0.1:3001</code>, you have to pass <code>3000:3001</code> to <code>ports</code>.</p> Projects <p>The default project runs apps locally. However, you can log into Hub and configure additional projects to run apps in a cloud account of your choice. </p> <p>Learn more \u2192</p>"},{"location":"docs/guides/apps/#stopping-a-run","title":"Stopping a run","text":"<p>To stop the app, click <code>Ctrl</code>+<code>C</code> while the <code>dstack run</code> command is running, or use the <code>dstack stop</code> command. <code>dstack</code> will automatically clean up any cloud resources  if they are used.</p>"},{"location":"docs/guides/apps/#configuring-resources","title":"Configuring resources","text":"<p>If your project is configured to run apps in the cloud, you can use the  <code>resources</code> property in the YAML file to  request hardware resources like memory, GPUs, and shared memory size.</p> <pre><code>workflows:\n- name: fastapi-gpu-i\nprovider: bash\nports:\n- 3000\ncommands:\n- pip install -r apps/requirements.txt\n- uvicorn apps.main:app --port 3000 --host 0.0.0.0\nresources:\ngpu:\ncount: 1\ninterruptible: true\n</code></pre> <p>The <code>interruptible</code> property tells <code>dstack</code> to utilize spot instances. Spot instances may be not always available. But when they are available, they are significantly cheaper.</p>"},{"location":"docs/guides/apps/#setting-up-the-environment","title":"Setting up the environment","text":"<p>You can use <code>pip</code> and <code>conda</code> executables to install packages and set up the environment.</p> <p>Use the <code>python</code> property to specify a version of Python for pre-installation. Otherwise, <code>dstack</code> uses the local version.</p>"},{"location":"docs/guides/apps/#using-docker","title":"Using Docker","text":"<p>To run the app with your custom Docker image, you can use the <code>docker</code> provider.</p> <pre><code>workflows:\n- name: fastapi-docker\nprovider: docker\nimage: python:3.11\nports:\n- 3000\ncommands:\n- pip install -r apps/requirements.txt\n- uvicorn apps.main:app --port 3000 --host 0.0.0.0\n</code></pre>"},{"location":"docs/guides/apps/#configuring-cache","title":"Configuring cache","text":"<p>Apps may download files like pre-trained models, external data, or Python packages. To avoid downloading them on each run, you can choose which paths to cache between runs. </p> <pre><code>workflows:\n- name: fastapi-cached\nprovider: bash\nports:\n- 3000\ncommands:\n- pip install -r apps/requirements.txt\n- uvicorn apps.main:app --port 3000 --host 0.0.0.0\ncache:\n- path: ~/.cache/pip\n</code></pre> <p>NOTE:</p> <p>Cache saves files in the configured storage and downloads them at startup. This improves performance and saves you  from data transfer costs.</p>"},{"location":"docs/guides/apps/#cleaning-up-the-cache","title":"Cleaning up the cache","text":"<p>To clean up the cache, use the <code>dstack prune cache</code> CLI command, followed by the name of the configuration.</p> <p>NOTE:</p> <p>Check out the <code>dstackai/dstack-examples</code> repo for source code and other examples.</p>"},{"location":"docs/guides/dev-environments/","title":"Dev environments","text":"<p>A dev environment is a virtual machine that includes the environment and an interactive IDE or notebook setup based on a pre-defined configuration.</p> <p>With <code>dstack</code>, you can define such configurations as code and launch your dev environments with a single command,  either locally or in any cloud.</p>"},{"location":"docs/guides/dev-environments/#creating-a-configuration-file","title":"Creating a configuration file","text":"<p>A configuration can be defined as a YAML file (under the <code>.dstack/workflows</code> directory).</p> <pre><code>workflows:\n- name: code-gpu\nprovider: code\nsetup:\n- pip install -r dev-environments/requirements.txt\nresources:\ngpu:\ncount: 1\n</code></pre> <p>The configuration  allows you to customize hardware resources, set up the Python environment, expose ports, configure cache, and more.</p>"},{"location":"docs/guides/dev-environments/#running-a-dev-environment","title":"Running a dev environment","text":"<p>Once a configuration is defined, you can run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run code-gpu\n\nRUN      WORKFLOW  SUBMITTED  STATUS     TAG\nshady-1  code-gpu   now        Submitted  \n\nStarting SSH tunnel...\n\nTo exit, press Ctrl+C.\n\nWeb UI available at http://127.0.0.1:10000/?tkn=4d9cc05958094ed2996b6832f899fda1\n</code></pre> <p>If we click the URL, it will open the dev environment.</p> <p></p> <p>For convenience, <code>dstack</code> uses an exact copy of the source code that is locally present in the folder where you use the <code>dstack</code> command.</p> Using .gitignore <p>If you don't want the dev environment to sync certain files (especially large local files that are not needed for the dev environment), feel free to add them to the <code>.gitignore</code> file. In this case, <code>dstack</code> will ignore them, even if you aren't using Git.</p> <p>If you configure a project to run dev environments in the cloud, <code>dstack</code> will automatically provision the required cloud resources, and forward ports of the dev environment to your local machine.</p> Projects <p>The default project runs dev environments locally. However, you can log into Hub and configure additional projects to run dev environments in a cloud account of your choice. </p> <p>Learn more \u2192</p>"},{"location":"docs/guides/dev-environments/#stopping-a-dev-environment","title":"Stopping a dev environment","text":"<p>To stop the dev environment, click <code>Ctrl</code>+<code>C</code> while the <code>dstack run</code> command is running, or use the <code>dstack stop</code> command. <code>dstack</code> will automatically clean up any cloud resources if they are used.</p>"},{"location":"docs/guides/dev-environments/#supported-ides","title":"Supported IDEs","text":"<p>Out of the box, <code>dstack</code> allows you to launch VS Code, JupyterLab, and Jupyter notebooks. All you have to do is to set the <code>provider</code> property in the YAML configuration accordingly:</p> <ul> <li><code>code</code> \u2013 VS Code</li> <li><code>lab</code> - JupyterLab</li> <li><code>notebook</code> \u2013 Jupyter notebook</li> </ul>"},{"location":"docs/guides/dev-environments/#connecting-via-ssh","title":"Connecting via SSH","text":"<p>Alternatively, you can use the <code>bash</code> provider with the  <code>ssh</code> property set to <code>true</code> to attach any desktop IDE to the dev environment via SSH.</p> <pre><code>  workflows:\n- name: ssh\nprovider: bash\nssh: true commands:\n- tail -f /dev/null\n</code></pre>"},{"location":"docs/guides/dev-environments/#configuring-resources","title":"Configuring resources","text":"<p>If your project is configured to run dev environments in the cloud, you can use the  <code>resources</code> property in the YAML  file to request hardware resources like memory, GPUs, and shared memory size. </p> <pre><code>workflows:\n- name: code-v100-spot\nprovider: code\nresources:\ngpu:\nname: V100\ncount: 8\ninterruptible: true\n</code></pre> <p>NOTE:</p> <p>The <code>interruptible</code> property instructs <code>dstack</code> to use spot instances, which may not always be available. However, when they are, they are significantly cheaper.</p>"},{"location":"docs/guides/dev-environments/#setting-up-the-environment","title":"Setting up the environment","text":"<p>You can use the <code>setup</code> property in the YAML file to pre-install packages or run other bash commands during environment startup.</p> <pre><code>workflows:\n- name: code-conda\nprovider: code\npython: 3.11\nsetup:\n- conda env create -f environment.yml\n</code></pre> <p>The <code>python</code> property specifies the version of Python. If not specified, <code>dstack</code> uses your current version.</p> <p>You can use <code>pip</code> and <code>conda</code> executables to install packages and set up the environment.</p> <p>NOTE:</p> <p>Refer to Configuring cache to speed up the setup process. </p>"},{"location":"docs/guides/dev-environments/#exposing-ports","title":"Exposing ports","text":"<p>If you intend to run web apps from the dev environment, specify the list of ports using the <code>ports</code> property.</p> <p><code>dstack</code> will automatically forward these ports to your local machine. You'll see the URLs to access each port in the output.</p>"},{"location":"docs/guides/dev-environments/#configuring-cache","title":"Configuring cache","text":"<p>To avoid downloading files (e.g. pre-trained models, external data, or Python packages) every time you restart your dev environment, you can selectively cache the desired paths between runs.</p> <pre><code>workflows:\n- name: code-cached\nprovider: code\ncache:\n- path: ~/.cache/pip\n- path: ./data\n- path: ./models\n</code></pre> <p>NOTE:</p> <p>Caching enhances performance and helps you avoid unnecessary data transfer costs. </p>"},{"location":"docs/guides/dev-environments/#cleaning-up-the-cache","title":"Cleaning up the cache","text":"<p>To clean up the cache, you can delete the files directly from the dev environment or use the  <code>dstack prune cache</code> CLI command, followed by the name of the configuration.</p> <p>NOTE:</p> <p>Check out the <code>dstackai/dstack-examples</code> repo for source code and other examples.</p>"},{"location":"docs/guides/pipelines/","title":"Pipelines","text":"<p>A pipeline is a set of pre-defined configurations that allow to process data, train or fine-tune models, do batch inference  or other tasks.</p> <p>With <code>dstack</code>, you can define such configurations as code and run your tasks with a single command, either locally or in any cloud.</p>"},{"location":"docs/guides/pipelines/#creating-a-configuration-file","title":"Creating a configuration file","text":"<p>A configuration can be defined as a YAML file (under the <code>.dstack/workflows</code> directory).</p> <pre><code>workflows:\n- name: train-mnist-gpu\nprovider: bash\ncommands:\n- pip install -r pipelines/requirements.txt\n- python pipelines/train.py\nartifacts:\n- path: ./lightning_logs\nresources:\ngpu:\ncount: 1\n</code></pre> <p>The configuration allows you to customize hardware resources, set up the Python environment, output artifacts,  expose ports, configure cache, and of course provide the commands to run.</p> <p>The artifacts are saved at the end of the run, such as when it's stopped or finished.</p>"},{"location":"docs/guides/pipelines/#running-a-pipeline","title":"Running a pipeline","text":"<p>Once a configuration is defined, you can run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run train-mnist-gpu\n\nRUN      WORKFLOW        SUBMITTED  STATUS     TAG\nshady-1  train-mnist-gpu  now        Submitted  \n\nProvisioning... It may take up to a minute. \u2713\n\nGPU available: True, used: True\n\nEpoch 1: [00:03&lt;00:00, 280.17it/s, loss=1.35, v_num=0]\n---&gt; 100%\n</code></pre> <p>For convenience, <code>dstack</code> uses an exact copy of the source code that is locally present in the folder where you use the <code>dstack</code> command.</p> Using .gitignore <p>If you don't want the pipeline to sync certain files (especially large local files that are not needed for the pipeline), feel free to add them to the <code>.gitignore</code> file. In this case, <code>dstack</code> will ignore them, even if you aren't using Git.</p> <p>If you configure a project to run pipelines in the cloud, <code>dstack</code> will automatically provision the required cloud resources. After the workflow is finished, <code>dstack</code> will automatically save output artifacts and clean up cloud resources.</p> Projects <p>The default project runs pipelines locally. However, you can log into Hub and configure additional projects to run pipelines in a cloud account of your choice. </p> <p>Learn more \u2192</p>"},{"location":"docs/guides/pipelines/#stopping-a-run","title":"Stopping a run","text":"<p>To stop the run, click <code>Ctrl</code>+<code>C</code> while the <code>dstack run</code> command is running, or use the <code>dstack stop</code> command.</p>"},{"location":"docs/guides/pipelines/#passing-arguments","title":"Passing arguments","text":"<p>To pass arguments to your pipeline, use the <code>${{ run.args }}</code> markup within the configuration:</p> <pre><code>workflows:\nname: train-mnist-args\nprovider: bash\ncommands:\n- pip install -r pipelines/requirements.txt\n- python pipelines/train.py ${{ run.args }}\nartifacts:\n- path: ./lightning_logs\n</code></pre> <p>This allows you to include arguments when executing the <code>dstack run</code> command for your pipeline:</p> <pre><code>$ dstack run train-mnist-gpu --batch-size 32\n</code></pre>"},{"location":"docs/guides/pipelines/#configuring-resources","title":"Configuring resources","text":"<p>If your project is configured to run pipelines in the cloud, you can use the  <code>resources</code> property in the YAML file to  request hardware resources like memory, GPUs, and shared memory size.</p> <pre><code>workflows:\n- name: train-mnist-v100-i\nprovider: bash\ncommands:\n- pip install -r pipelines/requirements.txt\n- python pipelines/train.py\nartifacts:\n- path: ./lightning_logs\nresources:\ngpu:\nname: V100\ninterruptible: true\n</code></pre> <p>NOTE:</p> <p>The <code>interruptible</code> property instructs <code>dstack</code> to use spot instances, which may not always be available. However, when they are, they are significantly cheaper.</p>"},{"location":"docs/guides/pipelines/#setting-up-the-environment","title":"Setting up the environment","text":"<p>You can use <code>pip</code> and <code>conda</code> executables to install packages and set up the environment.</p> <p>Use the <code>python</code> property to specify a version of Python for pre-installation. Otherwise, <code>dstack</code> uses the local version.</p>"},{"location":"docs/guides/pipelines/#using-docker","title":"Using Docker","text":"<p>To run the pipeline with your custom Docker image, you can use the <code>docker</code> provider.</p> <pre><code>workflows:\n- name: train-mnist-docker\nprovider: docker\nimage: python:3.11\ncommands:\n- pip install -r pipelines/requirements.txt\n- python pipelines/train.py\nartifacts:\n- path: ./lightning_logs\n</code></pre>"},{"location":"docs/guides/pipelines/#exposing-ports","title":"Exposing ports","text":"<p>If you want the pipeline to serve web apps, specify the list of ports via the  <code>ports</code> property.</p> <pre><code>workflows:\n- name: train-mnist-gpu\nprovider: bash\nports:\n- 6006\ncommands:\n- pip install -r requirements.txt\n- tensorboard --port 6006 --host 0.0.0.0 --logdir ./lightning_logs &amp;\n- python train.py\nartifacts:\n- path: ./lightning_logs\n</code></pre> <p><code>dstack</code> automatically forwards ports to your local machine. You'll see the URLs to access each port in the output.</p>"},{"location":"docs/guides/pipelines/#configuring-cache","title":"Configuring cache","text":"<p>When running a pipeline, you may need to download files like pre-trained models, external data, or Python packages. To avoid downloading them on each run of your pipeline, you can choose which paths to cache between runs. </p> <pre><code>workflows:\n- name: train-mnist-cached\nprovider: bash\ncommands:\n- pip install -r pipelines/requirements.txt\n- python pipelines/train.py\ncache:\n- path: ./data\n- path: ~/.cache/pip\nartifacts:\n- path: ./lightning_logs\nresources:\ngpu:\ncount: 1\n</code></pre> <p>NOTE:</p> <p>Cache saves files in the configured storage and downloads them at startup. This improves performance and saves you  from data transfer costs.</p>"},{"location":"docs/guides/pipelines/#cleaning-up-the-cache","title":"Cleaning up the cache","text":"<p>To clean up the cache, use the <code>dstack prune cache</code> CLI command, followed by the name of the configuration.</p> <p>NOTE:</p> <p>Check out the <code>dstackai/dstack-examples</code> repo for source code and other examples.</p>"},{"location":"docs/guides/projects/","title":"Projects","text":"<p>A project allows you to configure where to run dev environments, pipelines, and apps, as well as manage users that  access it.</p> <p>At startup, <code>dstack</code> sets up a default project for local execution. To run dev environments, pipelines, and apps in your desired cloud account (AWS, GCP, Azure, etc), you must create the corresponding project and configure the <code>dstack</code> CLI to use it.</p> <p>NOTE:</p> <p>You can configure multiple projects and switch between them using the <code>dstack</code> CLI.</p>"},{"location":"docs/guides/projects/#creating-projects","title":"Creating projects","text":"<p>To create a new project, log in to the Hub application, open the <code>Projects</code> page, and click the <code>Add</code> button.</p> <p></p> <p>Then, you need to select the type of backend where you want to provision infrastructure and store data, and provide the corresponding backend settings. For instructions specific to a particular cloud, please refer to the relevant sections below.</p> AWS <p>To use AWS, you will require an S3 bucket for storing state and artifacts, as well as credentials to access the  corresponding cloud services.</p> <p>Learn more \u2192</p> Azure <p>To use Azure, you will require a storage account for storing state and artifacts, as well as Azure AD app credentials to access the corresponding cloud services.</p> <p>Learn more \u2192</p> GCP <p>To use GCP, you will require a cloud bucket for storing state and artifacts, as well as a service account to access the corresponding cloud services.</p> <p>Learn more \u2192</p>"},{"location":"docs/guides/projects/#configuring-the-cli","title":"Configuring the CLI","text":"<p>Once you have created the project, you will find the <code>CLI</code> code snippet in its <code>Settings</code>. </p> <p></p> <p>Execute this code in the terminal to configure the project with the CLI.</p> <pre><code>$ dstack config --url http://localhost:3000 --project aws --token 34ccc68b-8579-44ff-8923-026619ddb20d\n</code></pre> <p>To use this project with the CLI, you need to pass its name using the <code>--project</code> argument in CLI commands (such as <code>dstack run</code>, <code>dstack init</code>, etc).</p> <p>NOTE:</p> <p>If you want to set the project as the default, add the <code>--default</code> flag to the <code>dstack config</code> command.</p>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<p>Configurations are YAML files that describe what you want to run with <code>dstack</code>. Configurations can be of two types: <code>dev-environment</code> and <code>task</code>.</p> <p>Filename</p> <p>The configuration file must be named with the suffix <code>.dstack.yml</code>. For example, you can name the configuration file <code>.dstack.yml</code> or <code>app.dstack.yml</code>. You can define these configurations anywhere within your project. </p> <p>Each folder may have one default configuration file named <code>.dstack.yml</code>.</p> <p>Below is a full reference of all available properties.</p> <ul> <li><code>type</code> - (Required) The type of the configurations. Can be <code>dev-environment</code> or <code>task</code>.</li> <li><code>setup</code> - (Optional) The list of bash commands to pre-build the environment.</li> <li><code>ide</code> - (Required if <code>type</code> is <code>dev-environment</code>). Can be <code>vscode</code>.</li> <li><code>ports</code> - (Optional) The list of port numbers to expose</li> <li><code>env</code> - (Optional) The list of environment variables (e.g. <code>PYTHONPATH=src</code>)</li> <li><code>image</code> - (Optional) The name of the Docker image (as an alternative or an addition to <code>setup</code>)</li> <li><code>registry_auth</code> - (Optional) Credentials to pull the private Docker image<ul> <li><code>username</code> - (Required) Username</li> <li><code>password</code> - (Required) Password or access token</li> </ul> </li> <li><code>commands</code> - (Required if <code>type</code> is <code>task</code>). The list of bash commands to run as a task</li> <li><code>python</code> - (Optional) The major version of Python to pre-install (e.g., <code>\"3.11\"\"</code>). Defaults to the current version installed locally.</li> <li><code>cache</code> - (Optional) The directories to be cached between runs</li> </ul>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Profiles configure the project to use and the resources required for the run.</p> <p>Profiles are defined in the <code>.dstack/profiles.yml</code> file within your project directory.</p> <p>Below is a full reference of all available properties.</p> <ul> <li><code>profiles</code> (Required) - The root property (of an <code>array</code> type)<ul> <li><code>name</code> - (Required) The name of the profile</li> <li><code>resources</code> - (Optional) The minimum required resources<ul> <li><code>memory</code> - (Optional) The minimum size of RAM memory (e.g., <code>\"16GB\"</code>). </li> <li><code>gpu</code> - (Optional) The minimum number of GPUs, their model name and memory<ul> <li><code>name</code> - (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, <code>\"A100\"</code>, etc)</li> <li><code>count</code> - (Optional) The minimum number of GPUs. Defaults to <code>1</code>.</li> <li><code>memory</code> (Optional) The minimum size of GPU memory (e.g., <code>\"16GB\"</code>)</li> </ul> </li> <li><code>shm_size</code> (Optional) The size of shared memory (e.g., <code>\"8GB\"</code>). If you are using parallel communicating   processes (e.g., dataloaders in PyTorch), you may need to configure this.</li> </ul> </li> <li><code>instance-type</code> - (Optional) The type of instance. Can be <code>auto</code>, <code>on-demand</code>, and <code>spot</code>. Defaults to <code>auto</code>.</li> </ul> </li> </ul>"},{"location":"docs/reference/backends/aws/","title":"AWS","text":"<p>The <code>AWS</code> backend type allows to provision infrastructure and store state and artifacts in an AWS account.</p> <p>Follow the step-by-step guide below to configure a project with this backend.</p>"},{"location":"docs/reference/backends/aws/#1-create-an-s3-bucket","title":"1. Create an S3 bucket","text":"<p>First have to create an S3 bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p> <p>NOTE:</p> <p>Make sure that the bucket is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/aws/#2-create-an-iam-user","title":"2. Create an IAM user","text":"<p>The next step is to create an IAM user and  grant this user permissions to perform actions on the <code>s3</code>, <code>logs</code>, <code>secretsmanager</code>, <code>ec2</code>, and <code>iam</code> services.</p> IAM policy template <p>If you'd like to limit the permissions to the most narrow scope, feel free to use the IAM policy template below.</p> <p>Replace <code>{bucket_name}</code> and <code>{bucket_name_under_score}</code> variables in the template below with the values that correspond to your S3 bucket.</p> <p>For <code>{bucket_name}</code>, use the name of the S3 bucket.  For <code>{bucket_name_under_score}</code>, use the same but with dash characters replaced to underscores  (e.g. if <code>{bucket_name}</code> is <code>my-awesome-project</code>, then  <code>{bucket_name_under_score}</code>  must be <code>my_awesome_project</code>.</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:PutObject\",\n\"s3:GetObject\",\n\"s3:DeleteObject\",\n\"s3:ListBucket\",\n\"s3:GetLifecycleConfiguration\",\n\"s3:PutLifecycleConfiguration\",\n\"s3:PutObjectTagging\",\n\"s3:GetObjectTagging\",\n\"s3:DeleteObjectTagging\",\n\"s3:GetBucketAcl\"\n],\n\"Resource\": [\n\"arn:aws:s3:::{bucket_name}\",\n\"arn:aws:s3:::{bucket_name}/*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"logs:DescribeLogGroups\"\n],\n\"Resource\": [\n\"arn:aws:logs:*:*:log-group:*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"logs:FilterLogEvents\",\n\"logs:TagLogGroup\",\n\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\"\n],\n\"Resource\": [\n\"arn:aws:logs:*:*:log-group:/dstack/jobs/{bucket_name}*:*\",\n\"arn:aws:logs:*:*:log-group:/dstack/runners/{bucket_name}*:*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"secretsmanager:UpdateSecret\",\n\"secretsmanager:GetSecretValue\",\n\"secretsmanager:CreateSecret\",\n\"secretsmanager:PutSecretValue\",\n\"secretsmanager:PutResourcePolicy\",\n\"secretsmanager:TagResource\",\n\"secretsmanager:DeleteSecret\"\n],\n\"Resource\": [\n\"arn:aws:secretsmanager:*:*:secret:/dstack/{bucket_name}/credentials/*\",\n\"arn:aws:secretsmanager:*:*:secret:/dstack/{bucket_name}/secrets/*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:DescribeInstanceTypes\",\n\"ec2:DescribeSecurityGroups\",\n\"ec2:DescribeSubnets\",\n\"ec2:DescribeImages\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeSpotInstanceRequests\",\n\"ec2:RunInstances\",\n\"ec2:CreateTags\",\n\"ec2:CreateSecurityGroup\",\n\"ec2:AuthorizeSecurityGroupIngress\",\n\"ec2:AuthorizeSecurityGroupEgress\"\n],\n\"Resource\": \"*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:CancelSpotInstanceRequests\",\n\"ec2:TerminateInstances\"\n],\n\"Resource\": \"*\",\n\"Condition\": {\n\"StringEquals\": {\n\"aws:ResourceTag/dstack_bucket\": \"{bucket_name}\"\n}\n}\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:GetRole\",\n\"iam:CreateRole\",\n\"iam:AttachRolePolicy\",\n\"iam:TagRole\"\n],\n\"Resource\": \"arn:aws:iam::*:role/dstack_role_{bucket_name_under_score}*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:CreatePolicy\",\n\"iam:TagPolicy\"\n],\n\"Resource\": \"arn:aws:iam::*:policy/dstack_policy_{bucket_name_under_score}*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:GetInstanceProfile\",\n\"iam:CreateInstanceProfile\",\n\"iam:AddRoleToInstanceProfile\",\n\"iam:TagInstanceProfile\",\n\"iam:PassRole\"\n],\n\"Resource\": [\n\"arn:aws:iam::*:instance-profile/dstack_role_{bucket_name_under_score}*\",\n\"arn:aws:iam::*:role/dstack_role_{bucket_name_under_score}*\"\n]\n}\n]\n}\n</code></pre>"},{"location":"docs/reference/backends/aws/#3-create-an-access-key","title":"3. Create an access key","text":"<p>Once the IAM user is created, go ahead and create an access key.</p> <p>NOTE:</p> <p>Once the access key is created, make sure to download the <code>.csv</code> file containing your IAM user's <code>Access key ID</code> and <code>Secret access key</code>.</p>"},{"location":"docs/reference/backends/aws/#4-create-a-project","title":"4. Create a project","text":"<p>Now that you have an access key, log in to the Hub, open the <code>Projects</code> page, click <code>Add</code>, and select <code>AWS</code> in the <code>Type</code> field.</p> <p></p>"},{"location":"docs/reference/backends/aws/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Access key ID</code> - (Required) The Access key ID to authenticate <code>dstack</code> </li> <li><code>Secret access key</code> - (Required) The Secret access key to authenticate <code>dstack</code></li> <li><code>Region</code> - (Required) The region where <code>dstack</code> will create provision infrastructure and store state and artifacts</li> <li><code>Bucket</code> - (Required) The S3 bucket to store state and artifacts (must be in the same region)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>Subnet</code> - (Optional) The EC2 subnet is required to provision infrastructure using a non-default VPC and subnet. If   not specified, dstack will use the default VPC and subnet.</li> </ul>"},{"location":"docs/reference/backends/aws/#5-configure-the-cli","title":"5. Configure the CLI","text":"<p>NOTE:</p> <p>Once you have created the project, feel free to use the CLI code snippet to configure it for use with the created project.</p> <p>Learn more \u2192</p>"},{"location":"docs/reference/backends/azure/","title":"Azure","text":"<p>The <code>Azure</code> backend type allows to provision infrastructure and store state and artifacts in an Azure account.</p> <p>Follow the step-by-step guide below to configure a project with this backend.</p>"},{"location":"docs/reference/backends/azure/#1-create-a-resource-group","title":"1. Create a resource group","text":"<p>First, create a new Azure resource group. All resource created by <code>dstack</code> will belong to this group.</p>"},{"location":"docs/reference/backends/azure/#2-create-a-storage-account","title":"2. Create a storage account","text":"<p>Next, create an Azure storage account in the newly created resource group. <code>dstack</code> will use this storage account to store metadata and artifacts.</p> <p>NOTE:</p> <p>Make sure that the storage account is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/azure/#3-create-an-azure-active-directory-app","title":"3. Create an Azure Active Directory app","text":"<p><code>dstack</code> needs an Azure Active Directory app credentials to authenticate with your Azure account. If the app is <code>Owner</code> of the subscription, <code>dstack</code> will automatically set up all the resources required to run workflows. It will also create a separate managed identity with fine-grained permissions to authenticate with your Azure account when running workflows. </p> <p>To create new application credentials using the Azure CLI, run:</p> <pre><code>az ad sp create-for-rbac --name dstack-app --role Owner --scopes /subscriptions/$SUBSCRIPTION_ID --query \"{ client_id: appId, client_secret: password, tenant_id: tenant }\"\n</code></pre>"},{"location":"docs/reference/backends/azure/#4-create-a-project","title":"4. Create a project","text":"<p>Now that you have the credentials set up, log in to the Hub, open the <code>Projects</code> page, click <code>Add</code>, and select <code>Azure</code> in the <code>Type</code> field.</p> <p></p> <p>It may take up to a minute to set up Azure resource after saving the project settings.</p>"},{"location":"docs/reference/backends/azure/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Tenant ID</code> - (Required) The Azure Tenant ID</li> <li><code>Client ID</code> - (Required) The Client ID to authenticate <code>dstack</code></li> <li><code>Client Secret</code> - (Required) The Client secret to authenticate <code>dstack</code></li> <li><code>Subscription ID</code> - (Required) The Azure Subscription ID</li> <li><code>Location</code> - (Required) The region where <code>dstack</code> will create provision infrastructure and store state and artifacts</li> <li><code>Storage account</code> - (Required) The Storage account to store state and artifacts (must be in the same region)</li> </ul>"},{"location":"docs/reference/backends/azure/#5-configure-the-cli","title":"5. Configure the CLI","text":"<p>NOTE:</p> <p>Once you have created the project, feel free to use the CLI code snippet to configure it for use with the created project.</p> <p>Learn more \u2192</p>"},{"location":"docs/reference/backends/gcp/","title":"GCP","text":"<p>The GCP backend type allows to provision infrastructure and store state and artifacts in a GCP account.</p> <p>Follow the step-by-step guide below to configure a project with this backend.</p>"},{"location":"docs/reference/backends/gcp/#1-enable-apis","title":"1. Enable APIs","text":"<p>First, ensure that the required APIs are enabled in your GCP project.</p> Required APIs <p>Here's the list of APIs that have to be enabled for the project.</p> <pre><code>cloudapis.googleapis.com\ncompute.googleapis.com \nlogging.googleapis.com\nsecretmanager.googleapis.com\nstorage-api.googleapis.com\nstorage-component.googleapis.com \nstorage.googleapis.com \n</code></pre>"},{"location":"docs/reference/backends/gcp/#2-create-a-storage-bucket","title":"2. Create a storage bucket","text":"<p>Once the APIs are enabled, proceed and create a storage bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p> <p>NOTE:</p> <p>Make sure that the bucket is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/gcp/#3-create-a-service-account","title":"3. Create a service account","text":"<p>The next step is to create a service account and configure the following roles for it: <code>Service Account User</code>, <code>Compute Admin</code>, <code>Storage Admin</code>, <code>Secret Manager Admin</code>, and <code>Logging Admin</code>.</p>"},{"location":"docs/reference/backends/gcp/#4-create-a-service-account-key","title":"4. Create a service account key","text":"<p>Once the service account is set up, create a key for it and download the corresponding JSON file.</p>"},{"location":"docs/reference/backends/gcp/#5-create-a-project","title":"5. Create a project","text":"<p>Now that you have a service account key, log in to the Hub, open the <code>Projects</code> page, click <code>Add</code>, and select <code>GCP</code> in the <code>Type</code> field.</p> <p></p>"},{"location":"docs/reference/backends/gcp/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Service account</code> - (Required) The JSON file of the Service account key to authenticate <code>dstack</code> </li> <li><code>Location</code> - (Required) The location where <code>dstack</code> will create provision infrastructure and store state and artifacts</li> <li><code>Region</code> - (Required) The region where <code>dstack</code> will create provision infrastructure and store state and artifacts</li> <li><code>Zone</code> - (Required) The zone where <code>dstack</code> will create provision infrastructure and store state and artifacts</li> <li><code>Bucket</code> - (Required) The Storage bucket to store state and artifacts (must be in the same region)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>Subnet</code> - (Optional) The EC2 subnet is required to provision infrastructure using a non-default VPC and subnet. If   not specified, dstack will use the default VPC and subnet.</li> </ul>"},{"location":"docs/reference/backends/gcp/#6-configure-the-cli","title":"6. Configure the CLI","text":"<p>NOTE:</p> <p>Once you have created the project, feel free to use the CLI code snippet to configure it for use with the created project.</p> <p>Learn more \u2192</p>"},{"location":"docs/reference/cli/config/","title":"dstack config","text":"<p>This command configures a Hub project.</p> <p>The configuration is stored in <code>~/.dstack/config.yaml</code>.</p>"},{"location":"docs/reference/cli/config/#usage","title":"Usage","text":"<pre><code>$ dstack config --help\nUsage: dstack config [-h] --url URL --project PROJECT --token TOKEN\n\nOptions:\n  --url URL           The URL of the Hub application, e.g. http://127.0.0.0.1\n  --project PROJECT   The name of the project to use as a remote\n  --token TOKEN       The personal access token of the Hub user\n</code></pre> <p>NOTE:</p> <p>You can configure multiple projects and use them interchangeably (by passing the <code>--project</code> argument to the <code>dstack  run</code> command. Any project can be set as the default by passing <code>--default</code> to the <code>dstack config</code> command.</p> <p>Configuring multiple projects can be convenient if you want to run workflows both locally and in the cloud or if  you would like to use multiple clouds.</p>"},{"location":"docs/reference/cli/config/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>--url URL</code> \u2013 (Required) The URL of the Hub application, e.g. <code>http://127.0.0.0.1</code></li> <li><code>--project PROJECT</code> \u2013 (Required) The name of the project to use as a remote</li> <li><code>--token TOKEN</code> \u2013 (Required) The personal access token of the Hub user</li> </ul>"},{"location":"docs/reference/cli/cp/","title":"dstack cp","text":"<p>The <code>cp</code> command copies artifact files to a local target path.</p>"},{"location":"docs/reference/cli/cp/#usage","title":"Usage","text":"<pre><code>$ dstack cp --help\nUsage: dstack cp [-h] (RUN | :TAG) SOURCE TARGET\n\nPositional Arguments:\n  (RUN | :TAG)  The name of the run or the tag\n  SOURCE        A path of an artifact file or directory\n  TARGET        A local path to download artifact file or directory into\n</code></pre>"},{"location":"docs/reference/cli/cp/#arguments-reference","title":"Arguments reference","text":"<p>One of the following arguments is required:</p> <ul> <li><code>RUN</code> \u2013 The name of the run</li> <li><code>:TAG</code> \u2013 The name of the tag</li> </ul> <p>The following arguments are required:</p> <ul> <li><code>SOURCE</code> \u2013 A path of an artifact file or directory</li> <li><code>TARGET</code> \u2013 A local path to download artifact file or directory into</li> </ul>"},{"location":"docs/reference/cli/init/","title":"dstack init","text":"<p>This command initializes the current directory as a repository.</p> <p>If the current repository is a Git repository, the command authorizes dstack to access it. By default, the command uses the default Git credentials configured for the repository. You can override these credentials by using arguments.</p> <p>The command also configures the SSH key that will be used for port forwarding and SSH access to running workflows. By default, it generates its own key. You can override this key by using the arguments. </p>"},{"location":"docs/reference/cli/init/#usage","title":"Usage","text":"<pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN] [--git-identity SSH_PRIVATE_KEY] [--ssh-identity SSH_PRIVATE_KEY]\n\nOptions:\n  --project PROJECT     The Hub project to execute the command for\n-t, --token OAUTH_TOKEN\n                        An authentication token for Git\n  --git-identity SSH_PRIVATE_KEY\n                        A path to the private SSH key file for non-public repositories\n  --ssh-identity SSH_PRIVATE_KEY\n                        A path to the private SSH key file for SSH port forwarding\n</code></pre>"},{"location":"docs/reference/cli/init/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are optional:</p> <ul> <li><code>-project PROJECT</code> \u2013 (Optional) The Hub project to execute the command for</li> <li><code>-t OAUTH_TOKEN</code>, <code>--token OAUTH_TOKEN</code> \u2013 (Optional) An authentication token for GitHub</li> <li><code>--git-identity SSH_PRIVATE_KEY</code> \u2013 (Optional) A path to the private SSH key file for non-public repositories</li> <li><code>--ssh-identity SSH_PRIVATE_KEY</code> \u2013 (Optional) A path to the private SSH key file for SSH port forwarding </li> </ul> <p>NOTE:</p> <p>If Git credentials are not passed via <code>--token OAUTH_TOKEN</code> or <code>--git-identity SSH_PRIVATE_KEY</code>, <code>dstack</code> uses the credentials configured in <code>~/.config/gh/hosts.yml</code> or <code>./.ssh/config</code> for the current Git repo.</p> <p>NOTE:</p> <p>If the project is configured to use the cloud, the credentials are stored in the encrypted cloud storage.</p>"},{"location":"docs/reference/cli/logs/","title":"dstack logs","text":"<p>This command shows the output of a given run within the current repository.</p>"},{"location":"docs/reference/cli/logs/#usage","title":"Usage","text":"<pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project PROJECT] [-a] [-s SINCE] RUN\n\nPositional Arguments:\n  RUN                   The name of the run\n\nOptional Arguments:\n  --project PROJECT     The name of the Hub project to execute the command for\n-a, --attach          Whether to continuously poll for new logs. By default, the command will\n                        exit once there are no more logs to display. To exit from this mode, use\n                        Control-C.\n  -s, --since SINCE     From what time to begin displaying logs. By default, logs will be\n                        displayed starting from 24 hours in the past. The value provided can be an\n                        ISO 8601 timestamp or a relative time. For example, a value of 5m would\n                        indicate to display logs starting five minutes in the past.\n</code></pre>"},{"location":"docs/reference/cli/logs/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>RUN</code> - (Required) The name of the run</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> - (Optional) The name of the Hub project to execute the command for</li> <li><code>-a</code>, <code>--attach</code> \u2013 (Optional) Whether to continuously poll for new logs while the workflow is still running.     By default, the command will exit once there are no more logs to display. To exit from this mode, use <code>Ctrl+C</code>.</li> <li><code>-s SINCE</code>, <code>--since SINCE</code> \u2013 (Optional) From what time to begin displaying logs. By default, logs will be displayed   starting from 24 hours in the past. The value provided can be an ISO 8601 timestamp or a   relative time. For example, a value of <code>5m</code> would indicate to display logs starting five   minutes in the past.</li> </ul>"},{"location":"docs/reference/cli/ls/","title":"dstack ls","text":"<p>The <code>ls</code> command lists the files of the artifacts of a given run or tag.</p>"},{"location":"docs/reference/cli/ls/#usage","title":"Usage","text":"<pre><code>$ dstack ls --help\nUsage: dstack ls [-h] [--project PROJECT] [-r] [-t] RUN | :TAG [SEARCH_PREFIX]\n\nPositional Arguments:\n  RUN | :TAG         The name of the run or the tag\n  SEARCH_PREFIX      Show files starting with prefix\n\nOptions:\n  -h, --help         Show this help message and exit\n--project PROJECT  Hub project to execute the command\n-r, --recursive    Show all files recursively\n</code></pre>"},{"location":"docs/reference/cli/ls/#arguments-reference","title":"Arguments reference","text":"<p>One of the following arguments is required:</p> <ul> <li><code>RUN</code> \u2013 The name of the run</li> <li><code>:TAG</code> \u2013 The name of the tag</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> \u2013 (Optional) The name of the Hub project to execute the command for</li> <li><code>-r</code>, <code>--recursive</code> \u2013 (Optional) Show all files recursively</li> <li><code>SEARCH_PREFIX</code> \u2013 (Optional) Show files starting with prefix</li> </ul>"},{"location":"docs/reference/cli/prebuild/","title":"dstack prebuild","text":"<p>This command create prebuild for a given configuration.</p>"},{"location":"docs/reference/cli/prebuild/#usage","title":"Usage","text":"<pre><code>$ dstack prebuild --help\nUsage: dstack prebuild [--project PROJECT] [--profile PROFILE] [-d] [--reload] WORKING_DIR [ARGS ...]\n\nPositional Arguments:\n  WORKING_DIR          The working directory of the run\n  ARGS                 Run arguments\n\nOptions:\n  --f FILE             The path to the run configuration file. Defaults to WORKING_DIR/.dstack.yml.\n  --project PROJECT    The name of the project\n  --profile PROFILE    The name of the profile\n</code></pre>"},{"location":"docs/reference/cli/prebuild/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>WORKING_DIR</code> - (Required) The working directory of the run (e.g. <code>.</code>)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>-f FILE</code>, <code>--f FILE</code> \u2013 (Optional) The path to the run configuration file. Defaults to <code>WORKING_DIR/.dstack.yml</code>.</li> <li><code>--project PROJECT</code> \u2013 (Optional) The name of the project</li> <li><code>--project PROJECT</code> \u2013 (Optional) The name of the profile</li> <li><code>ARGS</code> \u2013 (Optional) Use <code>ARGS</code> to pass custom run arguments</li> </ul>"},{"location":"docs/reference/cli/prune/","title":"dstack prune cache","text":"<p>The <code>prune</code> command prunes the cache of a specific workflow.</p>"},{"location":"docs/reference/cli/prune/#usage","title":"Usage","text":"<pre><code>$ dstack prune cache --help\n\nUsage: dstack prune cache [-h] [--project PROJECT] WORKFLOW\n\nPositional Arguments:\n  WORKFLOW    A workflow name to prune cache\n\nOptional Arguments:\n  --project PROJECT  The name of the Hub project to execute the command for\n</code></pre>"},{"location":"docs/reference/cli/prune/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>--project PROJECT</code> - (Optional) The name of the Hub project to execute the command for</li> <li><code>WORKFLOW</code> \u2014 (Required) A workflow name to prune cache</li> </ul>"},{"location":"docs/reference/cli/ps/","title":"dstack ps","text":"<p>This command shows status of runs within the current repository.</p>"},{"location":"docs/reference/cli/ps/#usage","title":"Usage","text":"<pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project PROJECT] [-a] [-v] [-w] [RUN]\n\nPositional Arguments:\n  RUN                The name of the run\n\nOptions:\n  --project PROJECT  The name of the Hub project to execute the command for\n-a, --all          Show all runs. By default, it only shows unfinished runs or the last finished.\n  -v, --verbose      Show more information about runs\n  -w, --watch        Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/ps/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are optional and mutually exclusive:</p> <ul> <li><code>-a</code>, <code>--all</code> \u2013 (Optional) Show all runs</li> <li><code>RUN</code> - (Optional) The name of the run</li> </ul> <p>NOTE:</p> <p>If <code>-a</code> is not used, the command shows only the statuses of active runs and the last finished one.</p> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> \u2013 (Optional) The name of the Hub project to execute the command for</li> <li><code>-v</code>, <code>--verbose</code> \u2013 (Optional) Show more information about runs</li> <li><code>-w</code>, <code>--watch</code> - (Optional) Watch statuses of runs in realtime</li> </ul>"},{"location":"docs/reference/cli/rm/","title":"dstack rm","text":"<p>Use this command to remove finished runs within the current repository.</p>"},{"location":"docs/reference/cli/rm/#usage","title":"Usage","text":"<pre><code>$ dstack rm --help\nUsage: dstack rm [-h] [--project PROJECT] [-a] [-y] [RUN]\n\nPositional Arguments:\n  RUN                The name of the run\n\nOptional Arguments:\n  --project PROJECT  The name of the Hub project to execute the command for\n-a, --all          Remove all finished runs\n  -y, --yes          Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/rm/#arguments-reference","title":"Arguments reference","text":"<p>One of the following arguments is required:</p> <ul> <li><code>RUN</code> - The name of a particular run</li> <li><code>-a</code>, <code>--all</code> \u2013 Remove all finished runs </li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> - (Optional) The name of the Hub project to execute the command for</li> <li><code>-y</code>, <code>--yes</code> \u2013 (Optional) Don't ask for confirmation </li> </ul>"},{"location":"docs/reference/cli/run/","title":"dstack run","text":"<p>This command runs a given configuration.</p>"},{"location":"docs/reference/cli/run/#usage","title":"Usage","text":"<pre><code>$ dstack run --help\nUsage: dstack run [--project PROJECT] [--profile PROFILE] [-d] [--reload] WORKING_DIR [ARGS ...]\n\nPositional Arguments:\n  WORKING_DIR          The working directory of the run\n  ARGS                 Run arguments\n\nOptions:\n  --f FILE             The path to the run configuration file. Defaults to WORKING_DIR/.dstack.yml.\n  --project PROJECT    The name of the project\n  --profile PROFILE    The name of the profile\n  -d, --detach         Do not poll logs and run status\n  --reload             Enable auto-reload\n  -t, --tag TAG        A tag name. Warning, if the tag exists, it will be overridden.\n</code></pre>"},{"location":"docs/reference/cli/run/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>WORKING_DIR</code> - (Required) The working directory of the run (e.g. <code>.</code>)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>-f FILE</code>, <code>--f FILE</code> \u2013 (Optional) The path to the run configuration file. Defaults to <code>WORKING_DIR/.dstack.yml</code>.</li> <li><code>--project PROJECT</code> \u2013 (Optional) The name of the project</li> <li><code>--project PROJECT</code> \u2013 (Optional) The name of the profile</li> <li><code>--reload</code> \u2013 (Optional) Enable auto-reload </li> <li><code>-d</code>, <code>--detach</code> \u2013 (Optional) Run in the detached mode. Means, the command doesn't   poll logs and run status.</li> <li><code>-p PORT [PORT ...]</code>, <code>--port PORT [PORT ...]</code> \u2013 (Optional) Requests ports or define mappings for them (<code>APP_PORT:LOCAL_PORT</code>)</li> <li><code>-t TAG</code>, <code>--tag TAG</code> \u2013 (Optional) A tag name. Warning, if the tag exists, it will be overridden.</li> <li><code>ARGS</code> \u2013 (Optional) Use <code>ARGS</code> to pass custom run arguments</li> </ul> <p>Prebuild policies:</p> <ul> <li><code>--no-prebuild</code> \u2013 Run <code>setup</code> first, then <code>commands</code> if any</li> <li><code>--use-prebuild</code> \u2013 Use prebuild if available, otherwise fallback to <code>no-prebuild</code></li> <li><code>--force-prebuild</code> \u2013 Create prebuild, save it, and run <code>commands</code></li> <li><code>--prebuild-only</code> \u2014 Create prebuild and save it</li> </ul> <p>NOTE:</p> <p>By default, it runs it in the attached mode, so you'll see the output in real-time.</p>"},{"location":"docs/reference/cli/secrets/","title":"dstack secrets","text":"<p>Secrets allow to use sensitive data within workflows (such as passwords or security tokens) without  hard-coding them inside the code. Secrets are passed to running workflows via environment variables.</p> <p>NOTE:</p> <p>If the project is configured to use the cloud, secrets are stored in the encrypted cloud storage.</p>"},{"location":"docs/reference/cli/secrets/#dstack-secrets-list","title":"dstack secrets list","text":"<p>The <code>dstack secrets list</code> command displays the names of the secrets configured in the current repository.</p>"},{"location":"docs/reference/cli/secrets/#usage","title":"Usage","text":"<pre><code>$ dstack secrets list\n</code></pre>"},{"location":"docs/reference/cli/secrets/#dstack-secrets-add","title":"dstack secrets add","text":"<p>The <code>dstack secrets add</code> command adds a new secret for the current repository.</p>"},{"location":"docs/reference/cli/secrets/#usage_1","title":"Usage","text":"<pre><code>$ dstack secrets add --help\nUsage: dstack secrets add [-h] [--project PROJECT] [-y] NAME [VALUE]\n\nPositional Arguments:\n  NAME               The name of the secret\n  VALUE              The value of the secret\n\nOptional Arguments:\n  --project PROJECT  The name of the Hub project to execute the command for\n-y, --yes          Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/secrets/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>NAME</code> \u2013 (Required) A name of the secret. Must be unique within the current repository.</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> - (Optional) The name of the Hub project to execute the command for</li> <li><code>-y</code>, <code>--yes</code> \u2013 (Optional) Don't ask for confirmation </li> <li><code>VALUE</code> \u2013 (Optional) The value of the secret. If not specified, dstack prompts the user to enter it via a masked input.</li> </ul>"},{"location":"docs/reference/cli/secrets/#dstack-secrets-delete","title":"dstack secrets delete","text":"<p>The <code>dstack secrets delete</code> command adds a new secret for the current repository.</p>"},{"location":"docs/reference/cli/secrets/#usage_2","title":"Usage","text":"<pre><code>$ dstack secrets delete --help\nusage: dstack secrets delete [-h] [--project PROJECT] [-y] NAME\n\nPositional Arguments:\n  NAME               The name of the secret\n\nOptional Arguments:\n  --project PROJECT  The name of the Hub project to execute the command for\n-y, --yes          Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/secrets/#arguments-reference_1","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>NAME</code> - (Required) A name of a secret</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> - (Optional) The name of the Hub project to execute the command for</li> <li><code>-y</code>, <code>--yes</code> \u2013 (Optional) Don't ask for confirmation </li> </ul>"},{"location":"docs/reference/cli/start/","title":"dstack start","text":"<p>This command starts the Hub server. The Hub is required to run workflows.</p>"},{"location":"docs/reference/cli/start/#usage","title":"Usage","text":"<pre><code>$ dstack start --help\nUsage: dstack start [-h] [--host HOST] [-p PORT] [-l LOG-LEVEL] [--token TOKEN]\n\nOptions:\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  --token TOKEN         The personal access token of the admin user. Is generated randomly by default.\n</code></pre> <p>NOTE:</p> <p>On the first run, this command creates the default project to run workflows locally and updates the local config  accordingly (<code>~/.dstack/config.yaml</code>).</p>"},{"location":"docs/reference/cli/start/#arguments-reference","title":"Arguments reference","text":"<p>The following arguments are optional:</p> <ul> <li><code>--host HOST</code> \u2013 (Optional) Bind socket to this host. Defaults to <code>127.0.0.1</code></li> <li><code>-p PORT</code>, <code>--port PORT</code> \u2013 (Optional) Bind socket to this port. Defaults to <code>3000</code>.</li> <li><code>--token TOKEN</code> \u2013 (Optional) The personal access token of the admin user. Is generated randomly by default.</li> </ul>"},{"location":"docs/reference/cli/stop/","title":"dstack stop","text":"<p>This command stops run(s) within the current repository.</p>"},{"location":"docs/reference/cli/stop/#usage","title":"Usage","text":"<pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project PROJECT] [-a] [-x] [-y] [RUN]\n\nPositional Arguments:\n  RUN                The name of the run\n\nOptional Arguments:\n  --project PROJECT  The name of the Hub project to execute the command for\n-a, --all          Stop all unfinished runs\n  -x, --abort        Don't wait for a graceful stop and abort the run immediately\n  -y, --yes          Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/stop/#arguments-reference","title":"Arguments reference","text":"<p>One of the following arguments is required:</p> <ul> <li><code>RUN</code> - The name of a particular run</li> <li><code>-a</code>, <code>--all</code> \u2013 Stop all unfinished runs </li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>--project PROJECT</code> - (Optional) The name of the Hub project to execute the command for</li> <li><code>-x</code>, <code>--abort</code> \u2013 (Optional) Don't wait for a graceful stop and abort the run immediately </li> <li><code>-y</code>, <code>--yes</code> \u2013 (Optional) Don't ask for confirmation </li> </ul>"},{"location":"docs/reference/cli/tags/","title":"dstack tags","text":""},{"location":"docs/reference/cli/tags/#dstack-tags-list","title":"dstack tags list","text":"<p>The <code>tags list</code> command lists tags.</p>"},{"location":"docs/reference/cli/tags/#usage","title":"Usage","text":"<pre><code>$ dstack tags list\n</code></pre>"},{"location":"docs/reference/cli/tags/#dstack-tags-add","title":"dstack tags add","text":"<p>The <code>tags add</code> command creates a new tag. A tag and its artifacts can be later added as a dependency in a workflow.</p> <p>There are two ways of creating a tag:</p> <ol> <li>Tag a finished run </li> <li>Upload local data</li> </ol>"},{"location":"docs/reference/cli/tags/#usage_1","title":"Usage","text":"<pre><code>$ dstack tags add --help\nUsage: dstack tags add [-h] [-a PATH] [-y] TAG [RUN]\n\nPositional Arguments:\n  TAG                   The name of the tag\n  RUN                   The name of the run\n\nOptions:\n  -a, --artifact PATH   A path to local directory to upload as an artifact\n  -y, --yes             Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/tags/#arguments-reference","title":"Arguments reference","text":"<p>The following argument is required:</p> <ul> <li><code>TAG</code> \u2013 (Required) The name of the tag.</li> </ul> <p>One of the following arguments is also required:</p> <ul> <li><code>RUN</code> \u2013 (Optional) The name of the run</li> <li><code>-a PATH</code>, <code>--artifact PATH</code> - (Optional) The path to the local folder to be uploaded as an artifact.</li> </ul>"},{"location":"docs/reference/cli/tags/#examples","title":"Examples:","text":"<p>Tag the finished run <code>wet-mangust-1</code> with the <code>some_tag_1</code> tag:</p> <pre><code>$ dstack tags add some_tag_1 wet-mangust-1\n</code></pre> <p>Uploading two local folders <code>./output1</code> and <code>./output2</code> to create the <code>some_tag_2</code> tag:</p> <pre><code>$ dstack tags add some_tag_2 -a ./output1 -a ./output2\n</code></pre> <p>NOTE:</p> <p>To list or download the artifacts of a tag, use the <code>dstack artifacts list :TAG</code> and  <code>dstack artifacts download :TAG</code> commands.</p>"},{"location":"docs/reference/cli/tags/#dstack-tags-delete","title":"dstack tags delete","text":"<p>The <code>tags delete</code> command deletes a given tag.</p>"},{"location":"docs/reference/cli/tags/#usage_2","title":"Usage","text":"<pre><code>$ dstack tags delete --help\nUsage: dstack tags delete [-h] [-y] TAG_NAME\n\nPositional Arguments:\n  TAG_NAME    The name of the tag\n\nOptional Arguments:\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/tags/#arguments-reference_1","title":"Arguments reference","text":"<p>The following arguments are required:</p> <ul> <li><code>TAG</code> - (Required) The name of the tag</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>-y</code>, <code>--yes</code> \u2013 (Optional) Don't ask for confirmation </li> </ul>"},{"location":"docs/reference/providers/bash/","title":"bash","text":"<p>The <code>bash</code> provider runs given bash commands. </p> <p>It comes with Python and Conda pre-installed, and allows to expose ports. </p> <p>If GPU is requested, the provider pre-installs the CUDA driver too.</p>"},{"location":"docs/reference/providers/bash/#usage-example","title":"Usage example","text":"<pre><code>workflows:\n- name: \"train\"\nprovider: bash\npython: 3.10\ncommands:\n- pip install requirements.txt\n- python src/train.py\nartifacts: - path: ./checkpoint\nresources:\ninterruptible: true\ngpu: 1\n</code></pre> <p>To run this workflow, use the following command:</p> <pre><code>$ dstack run train\n</code></pre>"},{"location":"docs/reference/providers/bash/#properties-reference","title":"Properties reference","text":"<p>The following properties are required:</p> <ul> <li><code>commands</code> - (Required) The shell commands to run</li> </ul> <p>The following properties are optional:</p> <ul> <li><code>python</code> - (Optional) The major version of Python</li> <li><code>env</code> - (Optional) The list of environment variables </li> <li><code>artifacts</code> - (Optional) The list of output artifacts</li> <li><code>resources</code> - (Optional) The hardware resources required by the workflow</li> <li><code>ports</code> - (Optional) The list of ports to expose</li> <li><code>working_dir</code> - (Optional) The path to the working directory</li> <li><code>ssh</code> - (Optional) Runs SSH server in the container if <code>true</code></li> <li><code>cache</code> - (Optional) The list of directories to cache between runs</li> </ul>"},{"location":"docs/reference/providers/bash/#artifacts","title":"artifacts","text":"<p>The list of output artifacts</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be saved as an output artifact</li> <li><code>mount</code> \u2013 (Optional) <code>true</code> if the artifact files must be saved in real-time.     Must be used only when real-time access to the artifacts is important.      For example, for storing checkpoints when interruptible instances are used, or for storing     event files in real-time (e.g. TensorBoard event files.)     By default, it's <code>false</code>.</li> </ul>"},{"location":"docs/reference/providers/bash/#resources","title":"resources","text":"<p>The hardware resources required by the workflow</p> <ul> <li><code>cpu</code> - (Optional) The number of CPU cores</li> <li><code>memory</code> (Optional) The size of RAM memory, e.g. <code>\"16GB\"</code></li> <li><code>gpu</code> - (Optional) The number of GPUs, their model name and memory</li> <li><code>shm_size</code> - (Optional) The size of shared memory, e.g. <code>\"8GB\"</code></li> <li><code>interruptible</code> - (Optional) <code>true</code> if you want the workflow to use interruptible instances.     By default, it's <code>false</code>.</li> </ul> <p>NOTE:</p> <p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch),  you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p>"},{"location":"docs/reference/providers/bash/#gpu","title":"gpu","text":"<p>The number of GPUs, their name and memory</p> <ul> <li><code>count</code> - (Optional) The number of GPUs</li> <li><code>memory</code> (Optional) The size of GPU memory, e.g. <code>\"16GB\"</code></li> <li><code>name</code> (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, etc)</li> </ul>"},{"location":"docs/reference/providers/bash/#cache","title":"cache","text":"<p>The list of directories to cache between runs</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be cached</li> </ul>"},{"location":"docs/reference/providers/bash/#more-examples","title":"More examples","text":""},{"location":"docs/reference/providers/bash/#ports","title":"Ports","text":"<p>If you'd like your workflow to expose ports, you have to specify the <code>ports</code> property with the list of ports to expose. You could specify a mapping <code>APP_PORT:LOCAL_PORT</code> or just <code>APP_PORT</code> \u2014 in this case dstack will choose available <code>LOCAL_PORT</code> for you.</p> <p>NOTE:</p> <p>Ports range <code>10000-10999</code> is reserved for dstack needs. However, you could remap them to different <code>LOCAL_PORT</code>s.</p> <pre><code>workflows:\n- name: app\nprovider: bash\nports:\n- 3000\ncommands: - pip install -r requirements.txt\n- gunicorn main:app --bind 0.0.0.0:3000\n</code></pre> <p>When running a workflow remotely, the <code>dstack run</code> command automatically forwards the defined ports from the remote machine to your local machine. This allows you to securely access applications running remotely from your local machine.</p>"},{"location":"docs/reference/providers/bash/#background-processes","title":"Background processes","text":"<p>Similar to the regular <code>bash</code> shell, the <code>bash</code> provider permits the execution of background processes. This can be achieved by appending <code>&amp;</code> to the respective command.</p> <p>Here's an example:</p> <pre><code>workflows:\n- name: train-with-tensorboard\nprovider: bash\nports:\n- 6006\ncommands:\n- pip install torchvision pytorch-lightning tensorboard\n- tensorboard --port 6006 --host 0.0.0.0 --logdir lightning_logs &amp;\n- python train.py\nartifacts:\n- path: lightning_logs\n</code></pre> <p>This example will run the <code>tensorboard</code> application in the background, enabling browsing of the logs of the training job while it is in progress.</p>"},{"location":"docs/reference/providers/code/","title":"code","text":"<p>The <code>code</code> provider launches a VS Code dev environment.</p> <p>It comes with Python and Conda pre-installed. </p> <p>If GPU is requested, the provider pre-installs the CUDA driver too.</p>"},{"location":"docs/reference/providers/code/#usage-example","title":"Usage example","text":"<pre><code>workflows:\n- name: ide\nprovider: code\nartifacts:\n- path: ./output\nresources:\ninterruptible: true\ngpu: 1\n</code></pre> <p>To run this workflow, use the following command:</p> <pre><code>$ dstack run ide\n</code></pre>"},{"location":"docs/reference/providers/code/#properties-reference","title":"Properties reference","text":"<p>The following properties are optional:</p> <ul> <li><code>setup</code> - (Optional) The list of shell commands to run before running the Python file</li> <li><code>python</code> - (Optional) The major version of Python</li> <li><code>environment</code> - (Optional) The list of environment variables </li> <li><code>artifacts</code> - (Optional) The list of output artifacts</li> <li><code>resources</code> - (Optional) The hardware resources required by the workflow</li> <li><code>ports</code> - (Optional) The list of ports to expose</li> <li><code>working_dir</code> - (Optional) The path to the working directory</li> <li><code>ssh</code> - (Optional) Runs SSH server in the container if <code>true</code> (by default)</li> <li><code>cache</code> - (Optional) The list of directories to cache between runs</li> </ul>"},{"location":"docs/reference/providers/code/#artifacts","title":"artifacts","text":"<p>The list of output artifacts</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be saved as an output artifact</li> <li><code>mount</code> \u2013 (Optional) <code>true</code> if the artifact files must be saved in real-time.     Must be used only when real-time access to the artifacts is important:      for storing checkpoints (e.g. if interruptible instances are used) and event files     (e.g. TensorBoard event files, etc.)     By default, it's <code>false</code>.</li> </ul>"},{"location":"docs/reference/providers/code/#resources","title":"resources","text":"<p>The hardware resources required by the workflow</p> <ul> <li><code>cpu</code> - (Optional) The number of CPU cores</li> <li><code>memory</code> (Optional) The size of RAM memory, e.g. <code>\"16GB\"</code></li> <li><code>gpu</code> - (Optional) The number of GPUs, their model name and memory</li> <li><code>shm_size</code> - (Optional) The size of shared memory, e.g. <code>\"8GB\"</code></li> <li><code>interruptible</code> - (Optional) <code>true</code> if you want the workflow to use interruptible instances.     By default, it's <code>false</code>.</li> </ul> <p>NOTE:</p> <p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch),  you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p>"},{"location":"docs/reference/providers/code/#gpu","title":"gpu","text":"<p>The number of GPUs, their name and memory</p> <ul> <li><code>count</code> - (Optional) The number of GPUs</li> <li><code>memory</code> (Optional) The size of GPU memory, e.g. <code>\"16GB\"</code></li> <li><code>name</code> (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, etc)</li> </ul>"},{"location":"docs/reference/providers/code/#cache","title":"cache","text":"<p>The list of directories to cache between runs</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be cached</li> </ul>"},{"location":"docs/reference/providers/code/#ports","title":"Ports","text":"<p>If you'd like your workflow to expose ports, you have to specify the <code>ports</code> property with the list of ports to expose. You could specify a mapping <code>APP_PORT:LOCAL_PORT</code> or just <code>APP_PORT</code> \u2014 in this case dstack will choose available <code>LOCAL_PORT</code> for you.</p> <p>NOTE:</p> <p>Ports range <code>10000-10999</code> is reserved for dstack needs. However, you could remap them to different <code>LOCAL_PORT</code>s.</p> <pre><code>workflows:\n- name: app\nprovider: code\nports:\n- 3000\nsetup: - pip install -r requirements.txt\n- gunicorn main:app --bind 0.0.0.0:3000\n</code></pre> <p>When running a workflow remotely, the <code>dstack run</code> command automatically forwards the defined ports from the remote machine to your local machine. This allows you to securely access applications running remotely from your local machine.</p>"},{"location":"docs/reference/providers/docker/","title":"docker","text":"<p>The <code>docker</code> provider runs given shell commands using a given Docker image.</p> <p>Unlike the <code>bash</code>, <code>code</code>, <code>lab</code>, and <code>notebook</code> providers, the <code>docker</code> provider doesn't pre-install Python, Conda, or the CUDA driver.</p> <p>If you plan to build your own Docker image, you can base it on the <code>dstackai/miniforge</code>  Docker image that has Conda and the CUDA driver pre-installed.</p>"},{"location":"docs/reference/providers/docker/#usage-example","title":"Usage example","text":"<pre><code>workflows:\n- name: hello-docker\nprovider: docker\nimage: ubuntu\nentrypoint: /bin/bash -i -c\ncommands:\n- mkdir -p output\n- echo 'Hello, world!' &gt; output/hello.txt\nartifacts:\n- path: ./output\nresources:\ngpu:\nname: \"K80\"\ncount: 1\n</code></pre> <p>To run this workflow, use the following command:</p> <pre><code>$ dstack run hello-docker\n</code></pre>"},{"location":"docs/reference/providers/docker/#properties-reference","title":"Properties reference","text":"<p>The following properties are required:</p> <ul> <li><code>image</code> - (Required) The Docker image name</li> </ul> <p>The following properties are optional:</p> <ul> <li><code>commands</code> - (Optional) The list of bash commands. If provided, the default image entrypoint is overridden</li> <li><code>entrypoint</code> - (Optional) The entrypoint string to override the image entrypoint</li> <li><code>version</code> - (Optional) The major version of Python</li> <li><code>environment</code> - (Optional) The list of environment variables </li> <li><code>artifacts</code> - (Optional) The list of output artifacts</li> <li><code>resources</code> - (Optional) The hardware resources required by the workflow</li> <li><code>ports</code> - (Optional) The list of ports to expose</li> <li><code>working_dir</code> - (Optional) The path to the working directory</li> <li><code>registry_auth</code> - (Optional) The private Docker registry credentials</li> <li><code>cache</code> - (Optional) The list of directories to cache between runs</li> </ul>"},{"location":"docs/reference/providers/docker/#artifacts","title":"artifacts","text":"<p>The list of output artifacts</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be saved as an output artifact</li> <li><code>mount</code> \u2013 (Optional) <code>true</code> if the artifact files must be saved in real-time.     Must be used only when real-time access to the artifacts is important:      for storing checkpoints (e.g. if interruptible instances are used) and event files     (e.g. TensorBoard event files, etc.)     By default, it's <code>false</code>.</li> </ul>"},{"location":"docs/reference/providers/docker/#resources","title":"resources","text":"<p>The hardware resources required by the workflow</p> <ul> <li><code>cpu</code> - (Optional) The number of CPU cores</li> <li><code>memory</code> (Optional) The size of RAM memory, e.g. <code>\"16GB\"</code></li> <li><code>gpu</code> - (Optional) The number of GPUs, their model name and memory</li> <li><code>shm_size</code> - (Optional) The size of shared memory, e.g. <code>\"8GB\"</code></li> <li><code>interruptible</code> - (Optional) <code>true</code> if you want the workflow to use interruptible instances.     By default, it's <code>false</code>.</li> </ul> <p>NOTE:</p> <p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch),  you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p>"},{"location":"docs/reference/providers/docker/#gpu","title":"gpu","text":"<p>The number of GPUs, their name and memory</p> <ul> <li><code>count</code> - (Optional) The number of GPUs</li> <li><code>memory</code> (Optional) The size of GPU memory, e.g. <code>\"16GB\"</code></li> <li><code>name</code> (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, etc)</li> </ul>"},{"location":"docs/reference/providers/docker/#cache","title":"cache","text":"<p>The list of directories to cache between runs</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be cached</li> </ul>"},{"location":"docs/reference/providers/docker/#registry_auth","title":"registry_auth","text":"<p>The private Docker registry credentials</p> <ul> <li><code>username</code> - The Docker registry username</li> <li><code>password</code> - The Docker registry password</li> </ul>"},{"location":"docs/reference/providers/docker/#more-examples","title":"More examples","text":""},{"location":"docs/reference/providers/docker/#ports","title":"Ports","text":"<p>If you'd like your workflow to expose ports, you have to specify the <code>ports</code> property with the list of ports to expose. You could specify a mapping <code>APP_PORT:LOCAL_PORT</code> or just <code>APP_PORT</code> \u2014 in this case dstack will choose available <code>LOCAL_PORT</code> for you.</p> <p>NOTE:</p> <p>Ports range <code>10000-10999</code> is reserved for dstack needs. However, you could remap them to different <code>LOCAL_PORT</code>s.</p> <pre><code>workflows:\n- name: hello-docker\nprovider: docker\nimage: python\nports:\n- 3000\ncommands:\n- pip install fastapi uvicorn\n- uvicorn usage.apps.hello_fastapi:app --port 3000 --host 0.0.0.0\n</code></pre> <p>When running a workflow remotely, the <code>dstack run</code> command automatically forwards the defined ports from the remote machine to your local machine. This allows you to securely access applications running remotely from your local machine.</p>"},{"location":"docs/reference/providers/docker/#private-docker-registry","title":"Private Docker registry","text":"<p>Below is an example of how to pass credentials to use an image from a private Docker registry:</p> <pre><code>workflows:\n- name: private-registry\nprovider: docker\nimage: ghcr.io/my-organization/top-secret-image:v1\nregistry_auth:\nusername: ${{ secrets.GHCR_USER }}\npassword: ${{ secrets.GHCR_TOKEN }}\n</code></pre> <p>NOTE:</p> <p>The <code>GHCR_USER</code> and <code>GHCR_TOKEN</code> secrets should be previously added via the secrets command.</p>"},{"location":"docs/reference/providers/lab/","title":"lab","text":"<p>The <code>lab</code> provider launches a JupyterLab dev environment.</p> <p>It comes with Python and Conda pre-installed, and allows to expose ports.</p> <p>If GPU is requested, the provider pre-installs the CUDA driver too. </p>"},{"location":"docs/reference/providers/lab/#usage-example","title":"Usage example","text":"<pre><code>workflows:\n- name: ide-lab\nprovider: lab\nartifacts: - path: ./output\nresources:\ninterruptible: true\ngpu: 1\n</code></pre> <p>To run this workflow, use the following command:</p> <pre><code>$ dstack run ide-lab\n</code></pre>"},{"location":"docs/reference/providers/lab/#properties-reference","title":"Properties reference","text":"<p>The following properties are optional:</p> <ul> <li><code>setup</code> - (Optional) The list of shell commands to run before running the JupyterLab application</li> <li><code>python</code> - (Optional) The major version of Python</li> <li><code>environment</code> - (Optional) The list of environment variables </li> <li><code>artifacts</code> - (Optional) The list of output artifacts</li> <li><code>resources</code> - (Optional) The hardware resources required by the workflow</li> <li><code>ports</code> - (Optional) The list of ports to expose</li> <li><code>working_dir</code> - (Optional) The path to the working directory</li> <li><code>ssh</code> - (Optional) Runs SSH server in the container if <code>true</code> (by default)</li> <li><code>cache</code> - (Optional) The list of directories to cache between runs</li> </ul>"},{"location":"docs/reference/providers/lab/#artifacts","title":"artifacts","text":"<p>The list of output artifacts</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be saved as an output artifact</li> <li><code>mount</code> \u2013 (Optional) <code>true</code> if the artifact files must be saved in real-time.     Must be used only when real-time access to the artifacts is important:      for storing checkpoints (e.g. if interruptible instances are used) and event files     (e.g. TensorBoard event files, etc.)     By default, it's <code>false</code>.</li> </ul>"},{"location":"docs/reference/providers/lab/#resources","title":"resources","text":"<p>The hardware resources required by the workflow</p> <ul> <li><code>cpu</code> - (Optional) The number of CPU cores</li> <li><code>memory</code> (Optional) The size of RAM memory, e.g. <code>\"16GB\"</code></li> <li><code>gpu</code> - (Optional) The number of GPUs, their model name and memory</li> <li><code>shm_size</code> - (Optional) The size of shared memory, e.g. <code>\"8GB\"</code></li> <li><code>interruptible</code> - (Optional) <code>true</code> if you want the workflow to use interruptible instances.     By default, it's <code>false</code>.</li> </ul> <p>NOTE:</p> <p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch),  you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p>"},{"location":"docs/reference/providers/lab/#gpu","title":"gpu","text":"<p>The number of GPUs, their name and memory</p> <ul> <li><code>count</code> - (Optional) The number of GPUs</li> <li><code>memory</code> (Optional) The size of GPU memory, e.g. <code>\"16GB\"</code></li> <li><code>name</code> (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, etc)</li> </ul>"},{"location":"docs/reference/providers/lab/#cache","title":"cache","text":"<p>The list of directories to cache between runs</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be cached</li> </ul>"},{"location":"docs/reference/providers/lab/#ports","title":"Ports","text":"<p>If you'd like your workflow to expose ports, you have to specify the <code>ports</code> property with the list of ports to expose. You could specify a mapping <code>APP_PORT:LOCAL_PORT</code> or just <code>APP_PORT</code> \u2014 in this case dstack will choose available <code>LOCAL_PORT</code> for you.</p> <p>NOTE:</p> <p>Ports range <code>10000-10999</code> is reserved for dstack needs. However, you could remap them to different <code>LOCAL_PORT</code>s.</p>"},{"location":"docs/reference/providers/notebook/","title":"notebook","text":"<p>The <code>notebook</code> provider launches a Jupyter notebook dev environment.</p> <p>It comes with Python and Conda pre-installed, and allows to expose ports.</p> <p>If GPU is requested, the provider pre-installs the CUDA driver too.</p>"},{"location":"docs/reference/providers/notebook/#usage-example","title":"Usage example","text":"<pre><code>workflows:\n- name: ide-notebook\nprovider: notebook\nresources:\ninterruptible: true\ngpu: 1\n</code></pre> <p>To run this workflow, use the following command:</p> <pre><code>$ dstack run ide-notebook\n</code></pre>"},{"location":"docs/reference/providers/notebook/#properties-reference","title":"Properties reference","text":"<p>The following properties are optional:</p> <ul> <li><code>setup</code> - (Optional) The list of shell commands to run before running the Notebook application</li> <li><code>python</code> - (Optional) The major version of Python</li> <li><code>environment</code> - (Optional) The list of environment variables </li> <li><code>artifacts</code> - (Optional) The list of output artifacts</li> <li><code>resources</code> - (Optional) The hardware resources required by the workflow</li> <li><code>ports</code> - (Optional) The list of ports to expose</li> <li><code>working_dir</code> - (Optional) The path to the working directory</li> <li><code>ssh</code> - (Optional) Runs SSH server in the container if <code>true</code> (by default)</li> <li><code>cache</code> - (Optional) The list of directories to cache between runs</li> </ul>"},{"location":"docs/reference/providers/notebook/#artifacts","title":"artifacts","text":"<p>The list of output artifacts</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be saved as an output artifact</li> <li><code>mount</code> \u2013 (Optional) <code>true</code> if the artifact files must be saved in real-time.     Must be used only when real-time access to the artifacts is important:      for storing checkpoints (e.g. if interruptible instances are used) and event files     (e.g. TensorBoard event files, etc.)     By default, it's <code>false</code>.</li> </ul>"},{"location":"docs/reference/providers/notebook/#resources","title":"resources","text":"<p>The hardware resources required by the workflow</p> <ul> <li><code>cpu</code> - (Optional) The number of CPU cores</li> <li><code>memory</code> (Optional) The size of RAM memory, e.g. <code>\"16GB\"</code></li> <li><code>gpu</code> - (Optional) The number of GPUs, their model name and memory</li> <li><code>shm_size</code> - (Optional) The size of shared memory, e.g. <code>\"8GB\"</code></li> <li><code>interruptible</code> - (Optional) <code>true</code> if you want the workflow to use interruptible instances.     By default, it's <code>false</code>.</li> </ul> <p>NOTE:</p> <p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch),  you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p>"},{"location":"docs/reference/providers/notebook/#gpu","title":"gpu","text":"<p>The number of GPUs, their name and memory</p> <ul> <li><code>count</code> - (Optional) The number of GPUs</li> <li><code>memory</code> (Optional) The size of GPU memory, e.g. <code>\"16GB\"</code></li> <li><code>name</code> (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, etc)</li> </ul>"},{"location":"docs/reference/providers/notebook/#cache","title":"cache","text":"<p>The list of directories to cache between runs</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be cached</li> </ul>"},{"location":"docs/reference/providers/notebook/#ports","title":"Ports","text":"<p>If you'd like your workflow to expose ports, you have to specify the <code>ports</code> property with the list of ports to expose. You could specify a mapping <code>APP_PORT:LOCAL_PORT</code> or just <code>APP_PORT</code> \u2014 in this case dstack will choose available <code>LOCAL_PORT</code> for you.</p> <p>NOTE:</p> <p>Ports range <code>10000-10999</code> is reserved for dstack needs. However, you could remap them to different <code>LOCAL_PORT</code>s.</p>"},{"location":"docs/reference/providers/ssh/","title":"ssh","text":"<p>The <code>ssh</code> provider runs ssh server inside the container and waits infinitely.</p> <p>It comes with Python and Conda pre-installed, and allows to expose ports. </p> <p>If GPU is requested, the provider pre-installs the CUDA driver too.</p>"},{"location":"docs/reference/providers/ssh/#usage-example","title":"Usage example","text":"<p>To run instance with 1 GPU and print connection URI for VS Code</p> <pre><code>$ dstack run ssh --gpu 1\n</code></pre>"},{"location":"docs/reference/providers/ssh/#properties-reference","title":"Properties reference","text":"<p>The following properties are optional:</p> <ul> <li><code>setup</code> - (Optional) The list of shell commands to run before idling</li> <li><code>python</code> - (Optional) The major version of Python</li> <li><code>env</code> - (Optional) The list of environment variables </li> <li><code>artifacts</code> - (Optional) The list of output artifacts</li> <li><code>resources</code> - (Optional) The hardware resources required by the workflow</li> <li><code>ports</code> - (Optional) The list of ports to expose</li> <li><code>working_dir</code> - (Optional) The path to the working directory</li> <li><code>cache</code> - (Optional) The list of directories to cache between runs</li> </ul>"},{"location":"docs/reference/providers/ssh/#artifacts","title":"artifacts","text":"<p>The list of output artifacts</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be saved as an output artifact</li> <li><code>mount</code> \u2013 (Optional) <code>true</code> if the artifact files must be saved in real-time.     Must be used only when real-time access to the artifacts is important.      For example, for storing checkpoints when interruptible instances are used, or for storing     event files in real-time (e.g. TensorBoard event files.)     By default, it's <code>false</code>.</li> </ul>"},{"location":"docs/reference/providers/ssh/#resources","title":"resources","text":"<p>The hardware resources required by the workflow</p> <ul> <li><code>cpu</code> - (Optional) The number of CPU cores</li> <li><code>memory</code> (Optional) The size of RAM memory, e.g. <code>\"16GB\"</code></li> <li><code>gpu</code> - (Optional) The number of GPUs, their model name and memory</li> <li><code>shm_size</code> - (Optional) The size of shared memory, e.g. <code>\"8GB\"</code></li> <li><code>interruptible</code> - (Optional) <code>true</code> if you want the workflow to use interruptible instances.     By default, it's <code>false</code>.</li> </ul> <p>NOTE:</p> <p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch),  you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p>"},{"location":"docs/reference/providers/ssh/#gpu","title":"gpu","text":"<p>The number of GPUs, their name and memory</p> <ul> <li><code>count</code> - (Optional) The number of GPUs</li> <li><code>memory</code> (Optional) The size of GPU memory, e.g. <code>\"16GB\"</code></li> <li><code>name</code> (Optional) The name of the GPU model (e.g. <code>\"K80\"</code>, <code>\"V100\"</code>, etc)</li> </ul>"},{"location":"docs/reference/providers/ssh/#cache","title":"cache","text":"<p>The list of directories to cache between runs</p> <ul> <li><code>path</code> \u2013 (Required) The relative path of the folder that must be cached</li> </ul>"},{"location":"docs/reference/providers/ssh/#more-examples","title":"More examples","text":"<p>See more examples at bash provider page.</p>"},{"location":"docs/usage/apps/","title":"Apps","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground. </p> <p>Both the <code>bash</code> and <code>docker</code> providers  allow workflows to host applications. To host apps within a workflow, you have to request the list of ports that your apps need.  Use the <code>ports</code> property for that.</p> <p>Create a Python script with a FastAPI application:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n</code></pre> <p>Now, define the following workflow YAML file:</p> <pre><code>workflows:\n- name: hello-fastapi\nprovider: bash\nports:\n- 3000\ncommands:\n- pip install fastapi uvicorn\n- uvicorn usage.apps.hello_fastapi:app --port 3000 --host 0.0.0.0\n</code></pre> <p>NOTE:</p> <p>Don't forget to bind your application to the <code>0.0.0.0</code> hostname.</p> <p>If you're running the workflow in the cloud, the <code>dstack run</code> command automatically forwards the defined ports from the remote machine to your local machine.</p> <pre><code>$ dstack run hello-fastapi\n RUN           WORKFLOW       SUBMITTED  STATUS     TAG  BACKENDS\n silly-dodo-1  hello-fastapi  now        Submitted       aws\n\nStarting SSH tunnel...\n\nTo interrupt, press Ctrl+C.\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:63475 (Press CTRL+C to quit)\n</code></pre> <p>This allows you to securely access applications running remotely from your local machine.</p>"},{"location":"docs/usage/args/","title":"Args","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground. </p> <p>If you pass any arguments to the <code>dstack run</code> command, they can be accessed from the workflow YAML file via the <code>${{ run.args }}</code> expression. </p> <p>Create the following Python script:</p> <pre><code>import sys\n\nif __name__ == '__main__':\n    print(sys.argv)\n</code></pre> <p>Then, define the following workflow YAML file:</p> <pre><code>workflows:\n- name: hello-args\nprovider: bash\ncommands:\n- python usage/args/hello_args.py ${{ run.args }}\n</code></pre> <p>Run it using <code>dstack run</code> and passing <code>\"Hello, world!\"</code> as an argument:</p> <pre><code>$ dstack run hello-args \"Hello, world!\"\n</code></pre> <p>NOTE:</p> <p>It supports any arguments except those that are reserved for the <code>dstack run</code> command.</p>"},{"location":"docs/usage/artifacts/","title":"Artifacts","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground. </p>"},{"location":"docs/usage/artifacts/#define-artifacts","title":"Define artifacts","text":"<p>Create the following workflow YAML file:</p> <pre><code>workflows:\n- name: hello-txt\nprovider: bash\ncommands:\n- echo \"Hello world\" &gt; output/hello.txt\nartifacts:\n- path: ./output\n</code></pre> <p>Run it using <code>dstack run</code>:</p> <pre><code>$ dstack run hello-txt\n</code></pre> <p>NOTE:</p> <p>Artifacts are saved at the end of the workflow. They are not saved if the workflow was aborted (e.g. via <code>dstack stop -x</code>).</p>"},{"location":"docs/usage/artifacts/#list-artifacts","title":"List artifacts","text":"<p>To see artifacts of a run, you can use the <code>dstack ls</code> command followed by the name of the run.</p> <pre><code>$ dstack ls -r grumpy-zebra-1\n\nPATH  FILE                                  SIZE\ndata  MNIST/raw/t10k-images-idx3-ubyte      7.5MiB\n      MNIST/raw/t10k-images-idx3-ubyte.gz   1.6MiB\n      MNIST/raw/t10k-labels-idx1-ubyte      9.8KiB\n      MNIST/raw/t10k-labels-idx1-ubyte.gz   4.4KiB\n      MNIST/raw/train-images-idx3-ubyte     44.9MiB\n      MNIST/raw/train-images-idx3-ubyte.gz  9.5MiB\n      MNIST/raw/train-labels-idx1-ubyte     58.6KiB\n      MNIST/raw/train-labels-idx1-ubyte.gz  28.2KiB    </code></pre>"},{"location":"docs/usage/artifacts/#add-tags","title":"Add tags","text":"<p>If you wish to reuse the artifacts of a specific run, you can assign a tag (via the <code>dstack tags</code> command)  to it and use the tag to reference the artifacts. </p> <p>Here's how to add a tag to a run:</p> <pre><code>$ dstack tags add grumpy-zebra-1 awesome-tag\n</code></pre> <p>Even if you delete the <code>grumpy-zebra-1</code> run, you can still access its artifacts using the <code>awesome-tag</code> tag name. </p>"},{"location":"docs/usage/artifacts/#real-time-artifacts","title":"Real-time artifacts","text":"<p>If you run your workflow remotely, and want to save artifacts in real time (as you write files to the disk),  you can set the <code>mount</code> property to <code>true</code> for a particular artifact.</p> <p>Let's create the following bash script:</p> <pre><code>for i in {000..100}\ndo\nsleep 1\necho $i &gt; \"output/${i}.txt\"\necho \"Wrote output/${i}.txt\"\ndone\n</code></pre> <p>Now, create the following workflow YAML file:</p> <pre><code>workflows:\n- name: hello-sh\nprovider: bash\ncommands:\n- bash artifacts/hello.sh\nartifacts:\n- path: ./output\nmount: true\n</code></pre> <p>Go ahead and run this workflow:</p> <pre><code>$ dstack run hello-sh --remote\n</code></pre> <p>NOTE:</p> <p>Every read or write operation within the mounted artifact directory will create an HTTP request to the storage.</p> <p>The <code>mount</code> option can be used to save and restore checkpoint files if the workflow uses interruptible instances.</p>"},{"location":"docs/usage/cache/","title":"Cache","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground.</p> <p>With <code>dstack</code>, you can cache folders for future runs of the same workflow, saving you from downloading the same data or packages repeatedly.</p> <p>Define the following workflow:</p> <pre><code>workflows:\n- name: hello-cache\nprovider: bash\ncommands:\n- pip install pandas\n- python usage/python/hello_pandas.py\ncache:\n- path: ~/.cache/pip\n</code></pre> <p>Run it using <code>dstack run</code>:</p> <pre><code>$ dstack run hello-cache\n</code></pre> <p>On the first run, Python packages will be downloaded. On subsequent runs, packages will be installed from the cache.</p>"},{"location":"docs/usage/cache/#prune-cache","title":"Prune cache","text":"<p>To clear the cache for a specific workflow, use the command <code>dstack prune cache</code> followed by the workflow name.</p> <pre><code>$ dstack prune cache zebra-1\n</code></pre>"},{"location":"docs/usage/deps/","title":"Deps","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground.</p> <p>By using <code>deps</code> workflows can reuse artifacts from other workflows. There are two methods for doing this: by specifying a workflow or tag name.</p>"},{"location":"docs/usage/deps/#workflows","title":"Workflows","text":"<p>Let's create the following workflow YAML file:</p> <pre><code>workflows:\n- name: cat-txt-2\nprovider: bash\ndeps:\n- workflow: hello-txt\ncommands:\n- cat output/hello.txt\n</code></pre> <p>If we run it, it will print the contents of the output artifacts of the last run of the <code>hello-txt</code> workflow.</p> <p>NOTE:</p> <p>Make sure to run the <code>hello-txt</code> workflow beforehand.</p>"},{"location":"docs/usage/deps/#tags","title":"Tags","text":"<p>Tags can be managed using the <code>dstack tags</code> command.</p> <p>You can create a tag either by assigning a tag name to a finished run or by uploading any local data.</p> <p>Say, you ran the <code>hello-txt</code> workflow, and want to reuse its artifacts in another workflow.</p> <p>Once the <code>hello-txt</code> workflow is finished, you can add a tag to it:</p> <pre><code>$ dstack tags add txt-file grumpy-zebra-2\n</code></pre> <p>The <code>txt-file</code> here is the name of the tag, and <code>grumpy-zebra-2</code> is the run name of the <code>hello-txt</code> workflow.</p> <p>Let's reuse the <code>txt-file</code> tag from another workflow:</p> <pre><code>workflows:\n- name: cat-txt\nprovider: bash\ndeps:\n- tag: txt-file\ncommands:\n- cat output/hello.txt\n</code></pre> <p>NOTE:</p> <p>You can create also a tag by uploading arbitrary local files. To do this, use the <code>dstack tags add</code> command  with the <code>-a PATH</code> argument, which should point to the local folder containing local files.</p>"},{"location":"docs/usage/deps/#external-repos","title":"External repos","text":"<p>By default, <code>dstack</code> looks up tags and workflows within the same repo.</p> <p>If you want to refer to a tag or a workflow from another repo, you have to prepend the name (of the tag or the workflow) with the repo name.</p> <p>The workflow below uses a tag from the <code>dstackai/dstack-playground</code> repo:</p> <pre><code>workflows:\n- name: cat-txt-3\nprovider: bash\ndeps:\n- workflow: dstackai/dstack-playground/txt-file\ncommands:\n- cat output/hello.txt\n</code></pre> <p>NOTE:</p> <p>Make sure to run the <code>hello-txt</code> workflow in the <code>dstackai/dstack</code> repo beforehand.</p>"},{"location":"docs/usage/dev-environments/","title":"Dev environments","text":"<p>Before creating a workflow that runs perfectly in the cloud, you may need to debug it. The <code>code</code>, <code>lab</code>, and <code>notebook</code> can help you do that by providing a way to launch interactive development environments.</p>"},{"location":"docs/usage/dev-environments/#vs-code","title":"VS Code","text":"<p>The <code>code</code> provider runs a VS Code application.</p> <pre><code>workflows:\n- name: ide-code\nprovider: code\n</code></pre> <p>If you run it, you'll see the URL:</p> <pre><code>$ dstack run ide-code\n\nRUN         WORKFLOW  SUBMITTED  STATUS     TAG  BACKEND\nlionfish-1  ide-code  now        Submitted       local\n\nStarting SSH tunnel...\n\nTo interrupt, press Ctrl+C.\n\nWeb UI available at http://127.0.0.1:51303/?tkn=f2de121b04054f1b85bb7c62b98f2de1\n</code></pre> <p>If you click the URL, it will open the VS Code application in the browser:</p> <p></p> <p>For more details, check the reference.</p> <p>NOTE:</p> <p>The <code>code</code> provider offers all the features that the <code>bash</code> provider does. This includes the ability to run both locally and remotely, set up Resources, run Python and Apps, use Deps,  Artifacts, etc.</p> <p>See Reference for more details.</p>"},{"location":"docs/usage/dev-environments/#notebooks","title":"Notebooks","text":"<p>Similar to <code>code</code>, the <code>lab</code> and <code>notebook</code> providers allow to run JupyterLab and Jupyter notebooks  correspondingly.</p> <p>See Reference for more details.</p>"},{"location":"docs/usage/dev-environments/#ssh","title":"SSH","text":"<p>Instead of using <code>code</code>, <code>lab</code>, and <code>notebook</code>, you can use the bash provider and connect your own IDE to the running workflow via SSH.</p> <p>Here's an example:</p> <pre><code>workflows:\n- name: hello-ssh\nprovider: bash\ncommands:\n- tail -f /dev/null\nssh: true </code></pre> <p>When you run this workflow, it will prompt you to connect to it via SSH.</p> <pre><code>$ dstack run hello-ssh\n RUN          WORKFLOW    SUBMITTED  STATUS     TAG  BACKENDS\n moody-emu-1  hello-ssh   now        Submitted       local\n\nStarting SSH tunnel...\n\nTo interrupt, press Ctrl+C.\n\nTo connect via SSH, use:\n  ssh -i /Users/cheptsov/.ssh/id_rsa root@localhost -p 56947\n</code></pre> <p>Now, you can use any IDE of your choice to connect to the running workflow.</p>"},{"location":"docs/usage/python/","title":"Python","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground. </p> <p>Create the following Python script:</p> <pre><code>if __name__ == '__main__':\n    print(\"Hello, world!\")\n</code></pre> <p>Then, create a workflow YAML file:</p> <pre><code>workflows:\n- name: hello-py\nprovider: bash\ncommands:\n- python usage/python/hello.py\n</code></pre> <p>Now, run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run hello-py\n\nRUN           WORKFLOW  SUBMITTED  STATUS     TAG  BACKEND \nshady-1       hello-py  now        Submitted       local\n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nHello, world\n</code></pre>"},{"location":"docs/usage/python/#python-packages","title":"Python packages","text":"<p>You can use <code>pip</code> within workflows install Python packages.</p> <p>Let's create the following Python script:</p> <pre><code>import pandas as pd\n\nif __name__ == '__main__':\n    df = pd.DataFrame(\n        {\n            \"Name\": [\n                \"Braund, Mr. Owen Harris\",\n                \"Allen, Mr. William Henry\",\n                \"Bonnell, Miss. Elizabeth\",\n            ],\n            \"Age\": [22, 35, 58],\n            \"Sex\": [\"male\", \"male\", \"female\"],\n        }\n    )\n\n    print(df)\n</code></pre> <p>Now, create the following workflow YAML file:</p> <pre><code>workflows:\n- name: hello-pandas\nprovider: bash\ncommands:\n- pip install pandas\n- python usage/python/hello_pandas.py\n</code></pre> <p>Run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run hello-pandas\n</code></pre>"},{"location":"docs/usage/python/#python-version","title":"Python version","text":"<p>By default, the workflow uses the same Python version that you use locally.  You can override the major Python version using the <code>python</code> property:</p> <pre><code>workflows:\n- name: python-version\nprovider: bash\npython: 3.7\ncommands:\n- python --version\n</code></pre> <p>Run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run python-version\n</code></pre>"},{"location":"docs/usage/python/#conda-packages","title":"Conda packages","text":"<p>You can use <code>conda</code> within workflows install Conda packages (under the hood, it uses Miniforge).</p> <p>Create the following Python script:</p> <pre><code>import pandas as pd\n\nif __name__ == '__main__':\n    df = pd.DataFrame(\n        {\n            \"Name\": [\n                \"Braund, Mr. Owen Harris\",\n                \"Allen, Mr. William Henry\",\n                \"Bonnell, Miss. Elizabeth\",\n            ],\n            \"Age\": [22, 35, 58],\n            \"Sex\": [\"male\", \"male\", \"female\"],\n        }\n    )\n\n    print(df)\n</code></pre> <p>Now, define a workflow YAML file:</p> <pre><code>workflows:\n- name: hello-conda\nprovider: bash\ncommands:\n- conda install pandas\n- python python/hello_pandas.py\n</code></pre> <p>Run it using the <code>dstack run</code> command:</p> <pre><code>$ dstack run hello-conda\n</code></pre>"},{"location":"docs/usage/python/#conda-environments","title":"Conda environments","text":"<p>You can create your custom Conda environments using <code>conda env create</code>,  save them as artifact, and reuse from other workflows via <code>deps</code> and <code>conda activate</code>.</p> <p>Say you have the following Conda environment YAML file:</p> <pre><code>name: myenv\n\ndependencies:\n- python=3.10\n- pandas\n</code></pre> <p>Now, create the following workflow YAML file:</p> <pre><code>workflows:\n- name: setup-conda\nprovider: bash\ncommands:\n- conda env create --file python/environment.yaml\nartifacts:\n- path: /opt/conda/envs/myenv\n\n- name: use-conda\nprovider: bash\ndeps:\n- workflow: setup-conda\ncommands:\n- conda activate myenv\n- conda env list\n</code></pre> <p>Now, run the <code>setup-conda</code> workflow:</p> <pre><code>$ dstack run setup-conda\n</code></pre> <p>And then, run the <code>use-conda</code> workflow:</p> <pre><code>$ dstack run use-conda\n\nconda environments:\n\nbase                     /opt/anaconda3\nworkflow                 /opt/conda/envs/workflow\nmyenv                *   /opt/conda/envs/myenv\n</code></pre> <p>The <code>use-conda</code> workflow reuses the <code>myenv</code> environment from the <code>setup-conda</code> workflow.</p> <p>NOTE:</p> <p>Conda environments are always bound to a specific architecture and cannot be reused across machines  with different architectures (e.g. <code>AMD64</code> vs <code>ARM64</code>).</p>"},{"location":"docs/usage/resources/","title":"Resources","text":"<p>When running a workflow in the cloud, you can specify which resources to use, such as GPU and memory.</p>"},{"location":"docs/usage/resources/#gpu","title":"GPU","text":"<p>If you run the following workflow remotely, <code>dstack</code> will automatically provision a machine with one <code>NVIDIA Tesla V100</code> GPU:</p> <pre><code>workflows:\n- name: gpu-v100\nprovider: bash\ncommands:\n- nvidia-smi\nresources:\ngpu:\nname: V100\ncount: 1\n</code></pre> <p>Go ahead, and run this workflow:</p> <pre><code>$ dstack run gpu-v100\n</code></pre> <p>NOTE:</p> <p>If you want to use GPU with your AWS account, make sure to have the corresponding service quota approved by the AWS support team beforehand. The approval typically takes a few business days.</p>"},{"location":"docs/usage/resources/#memory","title":"Memory","text":"<p>If you run the following workflow remotely, <code>dstack</code> will automatically provision a machine with 64GB memory:</p> <pre><code>workflows:\n- name: mem-64gb\nprovider: bash\ncommands:\n- free --giga\nresources:\nmemory: 64GB\n</code></pre> <p>Go ahead, and run this workflow:</p> <pre><code>$ dstack run mem-64gb\n</code></pre>"},{"location":"docs/usage/resources/#shared-memory","title":"Shared memory","text":"<p>If your workflow is using parallel communicating processes (e.g. dataloaders in PyTorch), you may need to configure the size of the shared memory (<code>/dev/shm</code> filesystem) via the <code>shm_size</code> property.</p> <p>The workflow below uses <code>16GB</code> of shared memory:</p> <pre><code>workflows:\n- name: shm-size\nprovider: bash\ncommands:\n- df /dev/shm\nresources:\nshm_size: 16GB </code></pre> <p>Try running this workflow:</p> <pre><code>$ dstack run shm-size\n</code></pre>"},{"location":"docs/usage/resources/#interruptible-instances","title":"Interruptible instances","text":"<p>Interruptible instances (also known as spot instances or preemptive instances) are offered at a significant price discount, and allow to use expensive machines at affordable prices.</p> <p>If you run the following workflow in the cloud, <code>dstack</code> will automatically provision a spot instance with one default GPU (<code>NVIDIA Tesla K80</code>):</p> <pre><code>workflows:\n- name: gpu-i\nprovider: bash\ncommands:\n- nvidia-smi\nresources:\ninterruptible: true\ngpu: 1\n</code></pre> <p>NOTE:</p> <p>If you want to use interruptible instances with your AWS account, make sure to have the corresponding service quota approved by the AWS support team beforehand. The approval typically takes a few business days.</p>"},{"location":"docs/usage/resources/#override-via-cli","title":"Override via CLI","text":"<p>Resources can be configured not only through the YAML file but also via the <code>dstack run</code> command.</p> <p>The following command that runs the <code>hello</code> workflow remotely using a spot instance with four GPUs:</p> <pre><code>$ dstack run hello --gpu 4 --interruptible\n</code></pre> <p>NOTE:</p> <p>To see all supported arguments (that can be used to override resources), use the <code>dstack run WORKFLOW --help</code> command.</p>"},{"location":"docs/usage/secrets/","title":"Secrets","text":"<p>NOTE:</p> <p>The source code of this example is available in the Playground.</p> <p>Secrets can be used to pass passwords and tokens securely to workflows without hard-coding them in the code.</p> <p>Secrets can be added via the <code>dstack secrets</code> command and accessed from the workflow via environment variables.</p>"},{"location":"docs/usage/secrets/#example","title":"Example","text":"<p>Here's an example of how to use your Weight &amp; Biases API token in your workflows.</p> <p>Go to the settings of your Weight &amp; Biases user and copy your API token.</p> <p>Use the <code>dstack secrets add</code> command to add it as a secret:</p> <pre><code>$ dstack secrets add WANDB_API_KEY acd0a9d1ebe3a4e4854d2f6a7cef85b5257f8183\n</code></pre> <p>Now, when you run any workflow, your API token will be passed to the workflow via the <code>WANDB_API_KEY</code> environment variable:</p> <pre><code>workflows:\n- name: hello\nprovider: bash\ncommands:\n- conda install wandb\n- wandb login\n</code></pre> <p>Secrets can be managed via the <code>dstack secrets</code> command.</p>"},{"location":"examples/dolly/","title":"Run your own ChatGPT with Dolly","text":"<p>This tutorial shows you how to run and debug your own ChatGPT on your cloud account using <code>dstack</code>, Gradio, and Dolly (Databricks' open-source pre-trained model).</p> <p>NOTE:</p> <p>The source code of this tutorial is available in the Playground.</p>"},{"location":"examples/dolly/#1-requirements","title":"1. Requirements","text":"<p>Here is the list of Python libraries that we will use:</p> <pre><code>transformers\naccelerate\ngradio\n</code></pre>"},{"location":"examples/dolly/#2-download-the-model","title":"2. Download the model","text":"<p>The heart of our ChatGPT application will be Dolly, the recently open-sourced pre-trained model. To use it in our application, we first have to download it from HuggingFace's Hub:</p> <pre><code>from huggingface_hub import snapshot_download\n\nmodel_name = \"databricks/dolly-v2-12b\"\n\nsnapshot_download(model_name, local_dir=f\".models/{model_name}\")\n</code></pre> <p>Once it's downloaded, you can load it to generate text given a prompt:</p> <pre><code>import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(\n    model=f\".models/{model_name}\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\nprint(generate_text(\"Explain how large language models work\"))\n</code></pre> <p>NOTE:</p> <p>In this tutorial, we use <code>databricks/dolly-v2-12b</code>, which is the largest variant of the model. Smaller options include <code>databricks/dolly-v2-3b</code> and <code>databricks/dolly-v2-7b</code>.  </p>"},{"location":"examples/dolly/#3-create-the-application","title":"3. Create the application","text":"<p>Now, let's put it all together into a simple chat application. To do that, we'll use Gradio and its built-in <code>Chatbot</code> component.</p> <p>Here's the complete code for our application:</p> <pre><code>import os\nfrom pathlib import Path\n\nimport gradio as gr\nimport torch\nfrom huggingface_hub import snapshot_download\nfrom transformers import pipeline\n\nmodel_name = \"databricks/dolly-v2-12b\"\n\nlocal_dir = f\"./models/{model_name}\"\nif not Path(local_dir).exists() or len(os.listdir(local_dir)) == 0:\n    snapshot_download(model_name, local_dir=local_dir)\n\ngenerate_text = pipeline(\n    model=local_dir,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\ntheme = gr.themes.Monochrome(\n    primary_hue=\"indigo\",\n    secondary_hue=\"blue\",\n    neutral_hue=\"slate\",\n    radius_size=gr.themes.sizes.radius_sm,\n    font=[\n        gr.themes.GoogleFont(\"Open Sans\"),\n        \"ui-sans-serif\",\n        \"system-ui\",\n        \"sans-serif\",\n    ],\n)\n\nwith gr.Blocks(theme=theme) as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        history[-1][1] = generate_text(history[-1][0])\n        return history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n        bot, chatbot, chatbot\n    )\n    clear.click(lambda: None, None, chatbot, queue=False)\n\nif __name__ == \"__main__\":\n    demo.launch(server_name=\"0.0.0.0\", server_port=3000)\n</code></pre>"},{"location":"examples/dolly/#4-define-the-workflow","title":"4. Define the workflow","text":"<p>To run our application in our cloud account using <code>dstack</code>, we need to define a <code>dstack</code> workflow as follows:</p> <pre><code>workflows:\n- name: dolly\nprovider: bash\nports:\n- 3000\ncommands:\n- pip install -r tutorials/dolly/requirements.txt\n- python tutorials/dolly/chat.py\ncache:\n- ~/.cache/pip\nresources:\ngpu:\nname: A100\nmemory: 60GB\ninterruptible: true\n</code></pre> <p>NOTE:</p> <p>We define a <code>dstack</code> workflow file to specify the requirements, script, ports, cached files, and hardware resources for our application. Our workflow requires an A100 GPU with at least 60GB of memory and interruptible (spot) instances if run in the cloud. </p>"},{"location":"examples/dolly/#5-run-the-workflow","title":"5. Run the workflow","text":"<p>NOTE:</p> <p>Before running the workflow, make sure that you have set up the Hub application and created a project that can run workflows in the cloud.</p> <p>Once the project is configured, we can use the <code>dstack run</code> command to run our workflow.</p> <p>NOTE:</p> <p>The Hub will automatically create the corresponding cloud resources, run the application, and forward the application port to localhost. If the workflow is completed, it automatically destroys the cloud resources.</p> <pre><code>$ dstack run dolly\n\nRUN       WORKFLOW  SUBMITTED  STATUS     TAG  BACKENDS\nturtle-1  dolly     now        Submitted       gcp\n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nDownloading model files: \n---&gt; 100%\n\nRunning on local URL:  http://127.0.0.1:3000\n</code></pre> <p>Clicking the URL from the output will open our ChatGPT application running in the cloud. </p> <p></p>"},{"location":"examples/dolly/#6-debug-the-workflow","title":"6. Debug the workflow","text":"<p>Before coming up with a workflow that runs perfectly in the cloud, you may need to debug it. With <code>dstack</code>, you can debug your workflow right in the cloud using an IDE. One way to do this is by using the <code>code</code> provider.</p> <p>Define the following workflow:</p> <pre><code>workflows:\n- name: debug-dolly\nprovider: code\nports:\n- 3000\nsetup:\n- pip install -r tutorials/dolly/requirements.txt\ncache:\n- ~/.cache/pip\nresources:\ngpu:\nname: A100\nmemory: 60GB\ninterruptible: true\n</code></pre> <p>If you run it, <code>dstack</code> will run a VS Code application with the code, dependencies, and hardware resources you've specified.</p> <pre><code>$ dstack run debug-dolly\n\nRUN        WORKFLOW     SUBMITTED  STATUS     TAG\nmangust-1  debug-dolly  now        Submitted     \n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nWeb UI available at http://127.0.0.1:3000\n</code></pre> <p>Clicking the last link will open VS Code on the provisioned machine.</p> <p></p> <p>You can run your code interactively, debug it, and run the application there. After fixing the workflow, you can run it using the <code>bash</code> provider.</p> <p>As an alternative to the <code>code</code> provider, you can run the <code>bash</code> provider with <code>ssh</code> set to <code>true</code>. This allows you to attach your own IDE to the running workflow.</p>"},{"location":"examples/github-actions/","title":"Trigger workflows with GitHub Actions","text":"<p>This tutorial demonstrates how to use GitHub Actions to trigger <code>dstack</code> workflows on commits, manually, scheduled time, or via REST API. </p> <p>GitHub Actions handle events, whereas <code>dstack</code> assists in provisioning the required cloud resources, executing the workflow, and managing artifacts.</p> <p>NOTE:</p> <p>The source code of this tutorial is available in the Playground.</p>"},{"location":"examples/github-actions/#1-prerequisites","title":"1. Prerequisites","text":"<p>To proceed, you will need a GitHub repository containing a workflow that you intend to trigger.  You may create a new repository or fork <code>dstackai/dstack-playground</code>.</p> <p>Below, we'll use the <code>train-mnist</code> workflow from <code>dstackai/dstack-playground</code>.</p>"},{"location":"examples/github-actions/#2-define-a-github-workflow","title":"2. Define a GitHub workflow","text":"<p>Create and push this GitHub workflow to the repository.</p> <pre><code>name: Train MNIST\n\non:\nworkflow_dispatch:\n\njobs:\ninit:\nruns-on: ubuntu-latest\nenv:\nAWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\nAWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\nsteps:\n- uses: actions/checkout@v3\n- uses: actions/setup-python@v4\n- name: Install dstack\nrun: |\npip install dstack\n- name: Configure the AWS remote\nrun: |\ndstack config aws --bucket ${{ secrets.DSTACK_AWS_S3_BUCKET }} --region ${{ secrets.DSTACK_AWS_REGION }}\n- name: Initialize dstack\nrun: |\nssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N \"\"\ndstack init --token ${{ secrets.GITHUB_TOKEN }}\n- name: Run dstack workflow\nrun: |\ndstack run train-mnist --remote --gpu 1\n</code></pre> <p>This workflow checks the source code, installs <code>dstack</code>, configures a remote for running <code>dstack</code> workflows (such as AWS or GCP), initializes <code>dstack</code> (needs <code>~/.ssh/id_rsa</code> to exist), and runs the workflow.</p> <p>NOTE:</p> <p>By default, <code>dstack</code> run runs the workflow in attached mode, which means it waits for the workflow to finish and streams real-time  output. To run the workflow in detached mode, add the <code>-d</code> flag with <code>dstack run</code> if needed.</p>"},{"location":"examples/github-actions/#3-configure-github-secrets","title":"3. Configure GitHub secrets","text":"<p>In order to run, our workflow requires several secrets t obe configured for your GitHub repository:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> \u2013 The access key ID of the AWS account to run the workflow</li> <li><code>AWS_SECRET_ACCESS_KEY</code> \u2013 The secret access key of the AWS account to run the workflow</li> <li><code>DSTACK_AWS_S3_BUCKET</code> - The name of the existing S3 bucket to use for saving artifacts</li> <li><code>DSTACK_AWS_REGION</code> \u2013 The AWS region to run the workflow</li> </ul>"},{"location":"examples/github-actions/#4-trigger-manually","title":"4. Trigger manually","text":"<p>As we're using the <code>workflow_dispatch</code> event, it's possible to manually trigger it via the user interface provided by GitHub.</p> <p></p>"},{"location":"examples/github-actions/#5-trigger-via-rest-api","title":"5. Trigger via REST API","text":"<p>Another option is to trigger it programmatically by using the REST API instead of manual triggering.</p> <pre><code>$ curl -L -X POST \\\n-H \"Accept: application/vnd.github+json\" \\\n-H \"Authorization: Bearer $GITHUB_TOKEN\" \\\nhttps://api.github.com/repos/dstackai/dstack-playground/actions/workflows/mnist.yml/dispatches \\\n-d '{\"ref\":\"main\"}'\n</code></pre>"},{"location":"examples/github-actions/#6-trigger-at-scheduled-time","title":"6. Trigger at scheduled time","text":"<p>The <code>on</code> property in the GitHub workflow file allows us to configure which events trigger the workflow.</p> <p>If we want to schedule the workflow to run at a specific time, we can use the following code.</p> <pre><code>on:\nschedule:\n- cron:  '30 5,17 * * *'\n</code></pre> <p>This expression will instruct GitHub to automatically trigger the workflow every day at 5:30 and 17:30 UTC.</p>"},{"location":"examples/stable-diffusion/","title":"Generate images with Stable Diffusion","text":"<p>This tutorial will show you how to run and debug a Gradio application on your cloud account that generates images using Stable Diffusion.</p> <p>NOTE:</p> <p>The source code of this tutorial is available in the Playground.</p>"},{"location":"examples/stable-diffusion/#1-requirements","title":"1. Requirements","text":"<p>Here is the list of Python libraries that we will use:</p> <pre><code>diffusers\ntransformers\nscipy\nftfy\naccelerate\nsafetensors\ngradio\n</code></pre> <p>NOTE:</p> <p>We're using the <code>safetensors</code> library because it implements a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy).</p>"},{"location":"examples/stable-diffusion/#2-download-the-model","title":"2. Download the model","text":"<p>In our tutorial, we'll use the <code>runwayml/stable-diffusion-v1-5</code> model (pretrained by Runway).</p> <pre><code>from huggingface_hub import snapshot_download\n\nmodel_name = \"runwayml/stable-diffusion-v1-5\"\n\nsnapshot_download(model_name, local_dir=f\".models/{model_name}\")\n</code></pre> <p>Once it's downloaded, you can load it to generate images based on a prompt:</p> <pre><code>from diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    f\"./models/{model_name}\", device_map=\"auto\", local_files_only=True\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"astronaut_rides_horse.png\")\n</code></pre>"},{"location":"examples/stable-diffusion/#3-create-the-application","title":"3. Create the application","text":"<p>Now, let's put it all together into a simple Gradio application.</p> <p>Here's the complete code.</p> <pre><code>import os\nfrom pathlib import Path\n\nimport gradio as gr\nfrom diffusers import StableDiffusionPipeline\nfrom huggingface_hub import snapshot_download\n\nmodel_name = \"runwayml/stable-diffusion-v1-5\"\n\nlocal_dir = f\"./models/{model_name}\"\nif not Path(local_dir).exists() or len(os.listdir(local_dir)) == 0:\n    snapshot_download(model_name, local_dir=local_dir)\n\npipe = StableDiffusionPipeline.from_pretrained(\n    f\"./models/{model_name}\", device_map=\"auto\", local_files_only=True\n)\n\ntheme = gr.themes.Monochrome(\n    primary_hue=\"indigo\",\n    secondary_hue=\"blue\",\n    neutral_hue=\"slate\",\n    radius_size=gr.themes.sizes.radius_sm,\n    font=[\n        gr.themes.GoogleFont(\"Open Sans\"),\n        \"ui-sans-serif\",\n        \"system-ui\",\n        \"sans-serif\",\n    ],\n)\n\nwith gr.Blocks(theme=theme) as demo:\n\n    def infer(prompt):\n        return pipe([prompt]).images\n\n    with gr.Row():\n        text = gr.Textbox(\n            show_label=False,\n            max_lines=1,\n            placeholder=\"Enter your prompt\",\n        ).style(container=False)\n        btn = gr.Button(\"Generate image\").style(full_width=False)\n\n    gallery = gr.Gallery(show_label=False).style(columns=[2], height=\"auto\")\n\n    text.submit(infer, inputs=text, outputs=[gallery])\n    btn.click(infer, inputs=text, outputs=[gallery])\n\nif __name__ == \"__main__\":\n    demo.launch(server_name=\"0.0.0.0\", server_port=3000)\n</code></pre>"},{"location":"examples/stable-diffusion/#4-define-the-workflow","title":"4. Define the workflow","text":"<p>To run our application in our cloud account using <code>dstack</code>, we need to define a <code>dstack</code> workflow as follows:</p> <pre><code>workflows:\n- name: stable-diffusion\nprovider: bash\nports:\n- 3000\ncommands:\n- pip install -r tutorials/stable_diffusion/requirements.txt\n- python tutorials/stable_diffusion/app.py\ncache:\n- ~/.cache/pip\nresources:\ngpu:\ncount: 1\nmemory: 16GB\ninterruptible: true\n</code></pre> <p>NOTE:</p> <p>We define a <code>dstack</code> workflow file to specify the requirements, script, ports, cached files, and hardware resources for our application. Our workflow requires a GPU and at least 16GB of memory and interruptible (spot) instances if run in the cloud. </p>"},{"location":"examples/stable-diffusion/#5-run-the-workflow","title":"5. Run the workflow","text":"<p>NOTE:</p> <p>Before running the workflow, make sure that you have set up the Hub application and created a project that can run workflows in the cloud.</p> <p>Once the project is configured, we can use the <code>dstack run</code> command to run our workflow.</p> <p>NOTE:</p> <p>The Hub will automatically create the corresponding cloud resources, run the application, and forward the application port to localhost. If the workflow is completed, it automatically destroys the cloud resources.</p> <pre><code>$ dstack run stable-diffusion\n\nRUN       WORKFLOW          SUBMITTED  STATUS     TAG\nturtle-1  stable-diffusion  now        Submitted     \n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nDownloading model files: \n---&gt; 100%\n\nRunning on local URL:  http://127.0.0.1:3000\n</code></pre> <p>Clicking the URL from the output will open our Gradio application running in the cloud. </p> <p></p>"},{"location":"examples/stable-diffusion/#6-debug-the-workflow","title":"6. Debug the workflow","text":"<p>Before coming up with a workflow that runs perfectly in the cloud, you may need to debug it. With <code>dstack</code>, you can debug your workflow right in the cloud using an IDE. One way to do this is by using the <code>code</code> provider.</p> <p>Define the following workflow:</p> <pre><code>workflows:\n- name: debug-stable-diffusion\nprovider: code\nports:\n- 3000\nsetup:\n- pip install -r tutorials/stable_diffusion/requirements.txt\ncache:\n- ~/.cache/pip\nresources:\ngpu:\ncount: 1\nmemory: 16GB\ninterruptible: true\n</code></pre> <p>If you run it, <code>dstack</code> will run a VS Code application with the code, dependencies, and hardware resources you've specified.</p> <pre><code>$ dstack run debug-stable-diffusion\n\nRUN        WORKFLOW                SUBMITTED  STATUS     TAG\nmangust-1  debug-stable-diffusion  now        Submitted     \n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nWeb UI available at http://127.0.0.1:3000\n</code></pre> <p>Clicking the last link will open VS Code on the provisioned machine.</p> <p></p> <p>You can run your code interactively, debug it, and run the application there. After fixing the workflow, you can run it using the <code>bash</code> provider.</p> <p>As an alternative to the <code>code</code> provider, you can run the <code>bash</code> provider with <code>ssh</code> set to <code>true</code>. This allows you to attach your own IDE to the running workflow.</p>"},{"location":"examples/tensorboard/","title":"Track experiments with TensorBoard","text":"<p>This tutorial demonstrates how to use TensorBoard with <code>dstack</code> to track experiment metrics.</p> <p>NOTE:</p> <p>The source code of this tutorial is available in the Playground.</p>"},{"location":"examples/tensorboard/#1-create-the-training-script","title":"1. Create the training script","text":"<p>TensorBoard is supported by all major training frameworks. Below, you will find the source code for a <code>tutorials/tensorboard/train.py</code> script that uses PyTorch Lightning to train the model and save the logs to the local lightning_logs folder.</p> Source code <p>To begin, we will create the training script <code>tutorials/tensorboard/train.py</code>.</p> <p>Our first step will be to define a <code>LightningDataModule</code>.</p> <pre><code>class MNISTDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        data_dir: Path = Path(\"data\"),\n        batch_size: int = 32,\n        num_workers: int = os.cpu_count(),\n    ):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n\n    def prepare_data(self):\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: Optional[str] = None):\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n\n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers\n        )\n</code></pre> <p>In order to log metrics at each epoch, we divide the data into <code>self.mnist_train</code> and <code>self.mnist_val</code>, and then we override the <code>val_dataloader</code> method to provide the validation dataloader that will be used during the validation step.</p> <p>We can now define the <code>LightningModule</code>.</p> <pre><code>class LitMNIST(pl.LightningModule):\n    def __init__(self, hidden_size=64, learning_rate=2e-4):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.learning_rate = learning_rate\n\n        self.num_classes = 10\n        channels, width, height = (1, 28, 28)\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy('multiclass', num_classes=10)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        return loss\n\ndef validation_step(self, batch, batch_idx):\nx, y = batch\nlogits = self(x)\nloss = F.cross_entropy(logits, y)\npreds = torch.argmax(logits, dim=1)\nself.accuracy(preds, y)\nself.log('val_loss', loss, prog_bar=True)\nself.log('val_acc', self.accuracy, prog_bar=True)\nreturn loss\ndef test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre> <p>We override the <code>validation_step</code> method to log validation metrics using the <code>self.log</code> method.</p> <p>Finally, let's define the <code>main</code> function that puts everything together.</p> <pre><code>def main():\n    dm = MNISTDataModule()\n    model = LitMNIST()\n\ntqdm_bar = TQDMProgressBar(refresh_rate=20)\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\ntrainer = pl.Trainer(\n        accelerator=\"auto\",\n        devices=\"auto\",\ncallbacks=[tqdm_bar, early_stop],\nmax_epochs=100,\n    )\n    trainer.fit(model, datamodule=dm)\n    trainer.test(datamodule=dm, ckpt_path='best')\n</code></pre> <p>For convenience, we utilize the <code>TQDMProgressBar</code> and <code>EarlyStopping</code> callbacks. The former provides interactive output, while the latter automatically stops training if the target metric no longer improves.</p>"},{"location":"examples/tensorboard/#2-define-the-workflow-file","title":"2. Define the workflow file","text":"<p>Create the following YAML file:</p> <pre><code>workflows:\n- name: train-tensorboard\nprovider: bash\nports:\n- port 6006\ncommands:\n- pip install torchvision pytorch-lightning tensorboard\n- tensorboard --port 6006 --host 0.0.0.0 --logdir ./lightning_logs &amp;\n- python tutorials/tensorboard/train.py\nartifacts:\n- path: ./lightning_logs\n</code></pre> <p>Here's what the workflow above does:</p> <ol> <li> <p>It launches the <code>tensorboard</code> application as a background process and direct it to the <code>lightning_logs</code> folder, where the training script will write event logs.</p> </li> <li> <p>The workflow requests a port from <code>dstack</code> and passes it to <code>tensorboard</code>. This allows us to access the TensorBoard application while the workflow is running.</p> </li> <li> <p>Finally, itssave the <code>lightning_logs</code> folder as an output artifact so that we can access the logs after the workflow has finished.</p> </li> </ol>"},{"location":"examples/tensorboard/#3-run-the-workflow","title":"3. Run the workflow","text":"<p>NOTE:</p> <p>Before running the workflow, make sure that you have set up the Hub application and created a project that can run workflows in the cloud.</p> <p>Once the project is configured, we can use the <code>dstack run</code> command to run our workflow.</p> <p>NOTE:</p> <p>The Hub will automatically create the corresponding cloud resources, run the application, and forward the application port to localhost. If the workflow is completed, it automatically destroys the cloud resources.</p> <pre><code>$ dstack run train-tensorboard\n\nRUN      WORKFLOW           SUBMITTED  STATUS     TAG  BACKENDS\nsnake-1  train-tensorboard  now        Submitted       local\n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nTensorBoard 2.12.0 at http://127.0.0.1:51623/ (Press CTRL+C to quit)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n---&gt; 100%\n\nGPU available: False, used: False\n\nEpoch 1:  0% 0/1719 [00:05&lt;00:09, 108.99it/s, v_num=0, val_loss=0.126, val_acc=0.964]\n...\n</code></pre> <p>By clicking on the TensorBoard URL in the output, we can monitor the metrics in real-time as the training progresses.</p> <p></p> <p>If you're running the workflow locally, its output artifacts are stored in the <code>~/.dstack/artifacts</code> directory. As a result, you can run the <code>tensorboard</code> CLI on your local machine and point it to the corresponding folder to view the logs at any time later.</p> <pre><code>$ tensorboard --logdir=~/.dstack/artifacts/github.com/dstack-playground/snake-1,train-tensorboard,0/lightning_logs\n</code></pre> <p>If you're running the workflow in the cloud, you can use the <code>dstack cp</code> command to copy remote artifacts to a  local folder:</p> <pre><code>$ tensorboard cp snake-1 ./artifacts\n\n---&gt; 100%\n</code></pre>"},{"location":"examples/wandb/","title":"Track experiments with W&amp;B","text":"<p>Using <code>dstack</code> with Weights &amp; Biases is a straightforward process.</p>"},{"location":"examples/wandb/#1-configure-the-api-key","title":"1. Configure the API key","text":"<p>To use the W&amp;B API in your workflow, you need to configure your API key.</p> <p>First, go to wandb.ai/authorize, copy the value, and add it to <code>dstack</code> as a secret:</p> <pre><code>$ dstack secrets add WANDB_API_KEY acd0a9d1ebe3a4e4854d2f6a7cef85b5257f8183 </code></pre> <p>Now, your API token will be passed to the workflow via the <code>WANDB_API_KEY</code> environment variable when you run any workflow.</p> <p>NOTE:</p> <p>Secrets are set up per repository, and they can only be used by workflows that run within that repo. </p> <p>You can test if it's working by using the following example:</p> <pre><code>workflows:\n- name: wandb-login\nprovider: bash\ncommands:\n- pip install wandb\n- wandb login\n</code></pre> <p>Run it to see if it works:</p> <pre><code>$ dstack run wandb-login\n\nRUN            WORKFLOW     SUBMITTED  STATUS     TAG  BACKENDS\ndull-turkey-1  wandb-login  now        Submitted       local\n\nProvisioning... It may take up to a minute. \u2713\n\nTo interrupt, press Ctrl+C.\n\nwandb: Currently logged in as: peterschmidt85. Use `wandb login --relogin` to force relogin\n\n$ </code></pre>"},{"location":"examples/wandb/#2-create-a-run","title":"2. Create a run","text":"<p>Now that you've checked that the API Key is configured, you can use the W&amp;B Python API to create a run and track metrics from your Python script.</p> <p>First, create a run with <code>wandb.init()</code>:</p> <pre><code>import os\n\nimport wandb\n\nwandb.init(project=\"my-awesome-project\", name=os.getenv(\"RUN_NAME\"))\n</code></pre> <p>NOTE:</p> <p>We're passing <code>os.getenv(\"RUN_NAME\")</code> which contains the name of our <code>dstack</code> run, to the W&amp;B run to match <code>dstack</code>'s run and W&amp;B's run.</p>"},{"location":"examples/wandb/#3-track-metrics","title":"3. Track metrics","text":"<p>Now, we can use <code>wandb.log()</code> and other APIs to track metrics from your training code:</p> <pre><code>wandb.log({'accuracy': train_acc, 'loss': train_loss})\n</code></pre> <p>Here's the workflow YAML file:</p> <pre><code>workflows:\n- name: wandb-init\nprovider: bash\ncommands:\n- pip install wandb\n- python examples/wandb/main.py\n</code></pre> <p>Running <code>wandb-init</code> will create the corresponding run in the W&amp;B user interface. If you tracked metrics  with <code>wandb.log()</code>, they would appear in real-time.</p>"}]}